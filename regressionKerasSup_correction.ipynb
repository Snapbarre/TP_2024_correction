{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 10:38:38.836311: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-12 10:38:38.839842: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-12 10:38:38.946487: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-12 10:38:38.948410: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-12 10:38:40.765002: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    }
   ],
   "source": [
    "dataset = loadtxt('dataset_diabetes.csv', delimiter=',')\n",
    "print(dataset.shape)\n",
    "x = dataset[:,:-1]\n",
    "y = dataset[:,-1:].reshape(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = int(0.8 * x.shape[0])\n",
    "x_train,x_val = x[:l,:],x[l:,:]\n",
    "y_train,y_val = y[:l],y[l:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_model(in_shape,out_shape=1):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_shape=in_shape,activation='relu'))\n",
    "    model.add(Dense(16, input_shape=(8,),activation='relu'))\n",
    "    model.add(Dense(out_shape, input_shape=(16,),activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "62/62 [==============================] - 2s 10ms/step - loss: 1.5436 - accuracy: 0.5358 - val_loss: 0.7410 - val_accuracy: 0.5909\n",
      "Epoch 2/150\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.8316 - accuracy: 0.6303 - val_loss: 0.7213 - val_accuracy: 0.6169\n",
      "Epoch 3/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.7435 - accuracy: 0.6254 - val_loss: 0.6544 - val_accuracy: 0.6364\n",
      "Epoch 4/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6672 - accuracy: 0.6661 - val_loss: 0.6489 - val_accuracy: 0.6494\n",
      "Epoch 5/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6716 - accuracy: 0.6694 - val_loss: 0.6614 - val_accuracy: 0.6753\n",
      "Epoch 6/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6290 - accuracy: 0.6889 - val_loss: 0.6411 - val_accuracy: 0.6364\n",
      "Epoch 7/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.6190 - accuracy: 0.6808 - val_loss: 0.6498 - val_accuracy: 0.6688\n",
      "Epoch 8/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.6092 - accuracy: 0.6873 - val_loss: 0.6236 - val_accuracy: 0.6364\n",
      "Epoch 9/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.6439 - accuracy: 0.6743 - val_loss: 0.6841 - val_accuracy: 0.6883\n",
      "Epoch 10/150\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.6023 - accuracy: 0.6971 - val_loss: 0.6375 - val_accuracy: 0.6039\n",
      "Epoch 11/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6015 - accuracy: 0.6808 - val_loss: 0.6323 - val_accuracy: 0.6818\n",
      "Epoch 12/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6036 - accuracy: 0.6792 - val_loss: 0.6661 - val_accuracy: 0.6818\n",
      "Epoch 13/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.6043 - accuracy: 0.6857 - val_loss: 0.6183 - val_accuracy: 0.6818\n",
      "Epoch 14/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5981 - accuracy: 0.7085 - val_loss: 0.6295 - val_accuracy: 0.6234\n",
      "Epoch 15/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5858 - accuracy: 0.7052 - val_loss: 0.6144 - val_accuracy: 0.7013\n",
      "Epoch 16/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5848 - accuracy: 0.7085 - val_loss: 0.6517 - val_accuracy: 0.5974\n",
      "Epoch 17/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5925 - accuracy: 0.7150 - val_loss: 0.6325 - val_accuracy: 0.6948\n",
      "Epoch 18/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5948 - accuracy: 0.7117 - val_loss: 0.6289 - val_accuracy: 0.6299\n",
      "Epoch 19/150\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5879 - accuracy: 0.6987 - val_loss: 0.6112 - val_accuracy: 0.6623\n",
      "Epoch 20/150\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5854 - accuracy: 0.7003 - val_loss: 0.6082 - val_accuracy: 0.6883\n",
      "Epoch 21/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5872 - accuracy: 0.7150 - val_loss: 0.6089 - val_accuracy: 0.6948\n",
      "Epoch 22/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5731 - accuracy: 0.7117 - val_loss: 0.6911 - val_accuracy: 0.6818\n",
      "Epoch 23/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5914 - accuracy: 0.6954 - val_loss: 0.5939 - val_accuracy: 0.7013\n",
      "Epoch 24/150\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.5767 - accuracy: 0.7117 - val_loss: 0.6032 - val_accuracy: 0.6753\n",
      "Epoch 25/150\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.5705 - accuracy: 0.7296 - val_loss: 0.6125 - val_accuracy: 0.6494\n",
      "Epoch 26/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5700 - accuracy: 0.7166 - val_loss: 0.6113 - val_accuracy: 0.6558\n",
      "Epoch 27/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5572 - accuracy: 0.7313 - val_loss: 0.6366 - val_accuracy: 0.6818\n",
      "Epoch 28/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5621 - accuracy: 0.7182 - val_loss: 0.6022 - val_accuracy: 0.6623\n",
      "Epoch 29/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5688 - accuracy: 0.7085 - val_loss: 0.6031 - val_accuracy: 0.6494\n",
      "Epoch 30/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5702 - accuracy: 0.7068 - val_loss: 0.6098 - val_accuracy: 0.6818\n",
      "Epoch 31/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5675 - accuracy: 0.7231 - val_loss: 0.5930 - val_accuracy: 0.6883\n",
      "Epoch 32/150\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.5554 - accuracy: 0.7280 - val_loss: 0.6086 - val_accuracy: 0.6753\n",
      "Epoch 33/150\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.5557 - accuracy: 0.7182 - val_loss: 0.5960 - val_accuracy: 0.6948\n",
      "Epoch 34/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5678 - accuracy: 0.7231 - val_loss: 0.6002 - val_accuracy: 0.6623\n",
      "Epoch 35/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5505 - accuracy: 0.7248 - val_loss: 0.6009 - val_accuracy: 0.6688\n",
      "Epoch 36/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5415 - accuracy: 0.7264 - val_loss: 0.6115 - val_accuracy: 0.7013\n",
      "Epoch 37/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5585 - accuracy: 0.7231 - val_loss: 0.5874 - val_accuracy: 0.6948\n",
      "Epoch 38/150\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5553 - accuracy: 0.7231 - val_loss: 0.6017 - val_accuracy: 0.6883\n",
      "Epoch 39/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5501 - accuracy: 0.7248 - val_loss: 0.6051 - val_accuracy: 0.6948\n",
      "Epoch 40/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5468 - accuracy: 0.7231 - val_loss: 0.6100 - val_accuracy: 0.6818\n",
      "Epoch 41/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5458 - accuracy: 0.7394 - val_loss: 0.5916 - val_accuracy: 0.6494\n",
      "Epoch 42/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5421 - accuracy: 0.7264 - val_loss: 0.5791 - val_accuracy: 0.7078\n",
      "Epoch 43/150\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.5468 - accuracy: 0.7410 - val_loss: 0.5867 - val_accuracy: 0.7078\n",
      "Epoch 44/150\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 0.5553 - accuracy: 0.7166 - val_loss: 0.6315 - val_accuracy: 0.6104\n",
      "Epoch 45/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5548 - accuracy: 0.7150 - val_loss: 0.5882 - val_accuracy: 0.6883\n",
      "Epoch 46/150\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.5473 - accuracy: 0.7182 - val_loss: 0.5894 - val_accuracy: 0.7013\n",
      "Epoch 47/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5485 - accuracy: 0.7394 - val_loss: 0.5859 - val_accuracy: 0.6948\n",
      "Epoch 48/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5405 - accuracy: 0.7345 - val_loss: 0.5833 - val_accuracy: 0.6883\n",
      "Epoch 49/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5354 - accuracy: 0.7296 - val_loss: 0.5971 - val_accuracy: 0.6818\n",
      "Epoch 50/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5665 - accuracy: 0.7362 - val_loss: 0.5896 - val_accuracy: 0.6948\n",
      "Epoch 51/150\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.5422 - accuracy: 0.7508 - val_loss: 0.5869 - val_accuracy: 0.7013\n",
      "Epoch 52/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5345 - accuracy: 0.7492 - val_loss: 0.5839 - val_accuracy: 0.7078\n",
      "Epoch 53/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5323 - accuracy: 0.7378 - val_loss: 0.5960 - val_accuracy: 0.6948\n",
      "Epoch 54/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5314 - accuracy: 0.7443 - val_loss: 0.6009 - val_accuracy: 0.6558\n",
      "Epoch 55/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5380 - accuracy: 0.7296 - val_loss: 0.5947 - val_accuracy: 0.6818\n",
      "Epoch 56/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5449 - accuracy: 0.7427 - val_loss: 0.5996 - val_accuracy: 0.6429\n",
      "Epoch 57/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5270 - accuracy: 0.7459 - val_loss: 0.5842 - val_accuracy: 0.6948\n",
      "Epoch 58/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5272 - accuracy: 0.7394 - val_loss: 0.5815 - val_accuracy: 0.7013\n",
      "Epoch 59/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5226 - accuracy: 0.7590 - val_loss: 0.5892 - val_accuracy: 0.6948\n",
      "Epoch 60/150\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 0.5269 - accuracy: 0.7524 - val_loss: 0.5977 - val_accuracy: 0.6883\n",
      "Epoch 61/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5222 - accuracy: 0.7410 - val_loss: 0.5796 - val_accuracy: 0.6948\n",
      "Epoch 62/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5315 - accuracy: 0.7476 - val_loss: 0.6243 - val_accuracy: 0.6948\n",
      "Epoch 63/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5356 - accuracy: 0.7280 - val_loss: 0.5976 - val_accuracy: 0.6753\n",
      "Epoch 64/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5199 - accuracy: 0.7508 - val_loss: 0.5842 - val_accuracy: 0.6883\n",
      "Epoch 65/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5210 - accuracy: 0.7378 - val_loss: 0.5901 - val_accuracy: 0.6364\n",
      "Epoch 66/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5230 - accuracy: 0.7492 - val_loss: 0.5753 - val_accuracy: 0.7208\n",
      "Epoch 67/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5426 - accuracy: 0.7296 - val_loss: 0.6186 - val_accuracy: 0.6429\n",
      "Epoch 68/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5280 - accuracy: 0.7459 - val_loss: 0.6029 - val_accuracy: 0.6753\n",
      "Epoch 69/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5158 - accuracy: 0.7557 - val_loss: 0.6250 - val_accuracy: 0.6818\n",
      "Epoch 70/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5235 - accuracy: 0.7459 - val_loss: 0.5825 - val_accuracy: 0.7013\n",
      "Epoch 71/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5149 - accuracy: 0.7557 - val_loss: 0.5891 - val_accuracy: 0.6883\n",
      "Epoch 72/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5351 - accuracy: 0.7410 - val_loss: 0.5902 - val_accuracy: 0.7013\n",
      "Epoch 73/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5161 - accuracy: 0.7459 - val_loss: 0.5732 - val_accuracy: 0.6948\n",
      "Epoch 74/150\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.5161 - accuracy: 0.7378 - val_loss: 0.5876 - val_accuracy: 0.6883\n",
      "Epoch 75/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5260 - accuracy: 0.7476 - val_loss: 0.5839 - val_accuracy: 0.6948\n",
      "Epoch 76/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5120 - accuracy: 0.7557 - val_loss: 0.5829 - val_accuracy: 0.6948\n",
      "Epoch 77/150\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 0.5220 - accuracy: 0.7638 - val_loss: 0.5785 - val_accuracy: 0.7143\n",
      "Epoch 78/150\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.5179 - accuracy: 0.7508 - val_loss: 0.6015 - val_accuracy: 0.6883\n",
      "Epoch 79/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5371 - accuracy: 0.7459 - val_loss: 0.5681 - val_accuracy: 0.7078\n",
      "Epoch 80/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5078 - accuracy: 0.7573 - val_loss: 0.5867 - val_accuracy: 0.6818\n",
      "Epoch 81/150\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5052 - accuracy: 0.7427 - val_loss: 0.5746 - val_accuracy: 0.7273\n",
      "Epoch 82/150\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5068 - accuracy: 0.7508 - val_loss: 0.5909 - val_accuracy: 0.6818\n",
      "Epoch 83/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5228 - accuracy: 0.7394 - val_loss: 0.5804 - val_accuracy: 0.7208\n",
      "Epoch 84/150\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5240 - accuracy: 0.7557 - val_loss: 0.5943 - val_accuracy: 0.6948\n",
      "Epoch 85/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5044 - accuracy: 0.7541 - val_loss: 0.5843 - val_accuracy: 0.6948\n",
      "Epoch 86/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5041 - accuracy: 0.7524 - val_loss: 0.5848 - val_accuracy: 0.6753\n",
      "Epoch 87/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4996 - accuracy: 0.7573 - val_loss: 0.5946 - val_accuracy: 0.6883\n",
      "Epoch 88/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5173 - accuracy: 0.7622 - val_loss: 0.5812 - val_accuracy: 0.7143\n",
      "Epoch 89/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5049 - accuracy: 0.7524 - val_loss: 0.5729 - val_accuracy: 0.7013\n",
      "Epoch 90/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5275 - accuracy: 0.7557 - val_loss: 0.6342 - val_accuracy: 0.6818\n",
      "Epoch 91/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5267 - accuracy: 0.7459 - val_loss: 0.5801 - val_accuracy: 0.7013\n",
      "Epoch 92/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5003 - accuracy: 0.7573 - val_loss: 0.5751 - val_accuracy: 0.7078\n",
      "Epoch 93/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5060 - accuracy: 0.7573 - val_loss: 0.5654 - val_accuracy: 0.6883\n",
      "Epoch 94/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4984 - accuracy: 0.7606 - val_loss: 0.5856 - val_accuracy: 0.7078\n",
      "Epoch 95/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4885 - accuracy: 0.7866 - val_loss: 0.5699 - val_accuracy: 0.7078\n",
      "Epoch 96/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4975 - accuracy: 0.7785 - val_loss: 0.5878 - val_accuracy: 0.7013\n",
      "Epoch 97/150\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4987 - accuracy: 0.7704 - val_loss: 0.5731 - val_accuracy: 0.6753\n",
      "Epoch 98/150\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.4973 - accuracy: 0.7573 - val_loss: 0.5560 - val_accuracy: 0.7013\n",
      "Epoch 99/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4970 - accuracy: 0.7671 - val_loss: 0.5749 - val_accuracy: 0.7078\n",
      "Epoch 100/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4921 - accuracy: 0.7671 - val_loss: 0.5823 - val_accuracy: 0.6948\n",
      "Epoch 101/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4942 - accuracy: 0.7492 - val_loss: 0.5715 - val_accuracy: 0.7208\n",
      "Epoch 102/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5002 - accuracy: 0.7590 - val_loss: 0.5731 - val_accuracy: 0.7078\n",
      "Epoch 103/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4927 - accuracy: 0.7524 - val_loss: 0.5925 - val_accuracy: 0.7013\n",
      "Epoch 104/150\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.4944 - accuracy: 0.7557 - val_loss: 0.5666 - val_accuracy: 0.7078\n",
      "Epoch 105/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4885 - accuracy: 0.7622 - val_loss: 0.5737 - val_accuracy: 0.6948\n",
      "Epoch 106/150\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.4871 - accuracy: 0.7638 - val_loss: 0.5702 - val_accuracy: 0.7208\n",
      "Epoch 107/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5011 - accuracy: 0.7524 - val_loss: 0.5757 - val_accuracy: 0.7078\n",
      "Epoch 108/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4934 - accuracy: 0.7622 - val_loss: 0.5692 - val_accuracy: 0.6688\n",
      "Epoch 109/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5011 - accuracy: 0.7573 - val_loss: 0.5586 - val_accuracy: 0.7273\n",
      "Epoch 110/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.4914 - accuracy: 0.7573 - val_loss: 0.5583 - val_accuracy: 0.7143\n",
      "Epoch 111/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.4831 - accuracy: 0.7687 - val_loss: 0.5944 - val_accuracy: 0.7078\n",
      "Epoch 112/150\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7606 - val_loss: 0.5665 - val_accuracy: 0.7143\n",
      "Epoch 113/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5048 - accuracy: 0.7557 - val_loss: 0.5728 - val_accuracy: 0.7013\n",
      "Epoch 114/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4915 - accuracy: 0.7671 - val_loss: 0.5584 - val_accuracy: 0.7013\n",
      "Epoch 115/150\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.4851 - accuracy: 0.7557 - val_loss: 0.5619 - val_accuracy: 0.7143\n",
      "Epoch 116/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4839 - accuracy: 0.7769 - val_loss: 0.5981 - val_accuracy: 0.6883\n",
      "Epoch 117/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4989 - accuracy: 0.7655 - val_loss: 0.5704 - val_accuracy: 0.7208\n",
      "Epoch 118/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.4900 - accuracy: 0.7622 - val_loss: 0.5724 - val_accuracy: 0.7013\n",
      "Epoch 119/150\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4780 - accuracy: 0.7736 - val_loss: 0.5917 - val_accuracy: 0.7078\n",
      "Epoch 120/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4864 - accuracy: 0.7736 - val_loss: 0.5992 - val_accuracy: 0.6948\n",
      "Epoch 121/150\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.4903 - accuracy: 0.7736 - val_loss: 0.6352 - val_accuracy: 0.6623\n",
      "Epoch 122/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5019 - accuracy: 0.7427 - val_loss: 0.5591 - val_accuracy: 0.6948\n",
      "Epoch 123/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4837 - accuracy: 0.7590 - val_loss: 0.5553 - val_accuracy: 0.7208\n",
      "Epoch 124/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4777 - accuracy: 0.7769 - val_loss: 0.5627 - val_accuracy: 0.6753\n",
      "Epoch 125/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4986 - accuracy: 0.7573 - val_loss: 0.5828 - val_accuracy: 0.6818\n",
      "Epoch 126/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.4877 - accuracy: 0.7638 - val_loss: 0.5625 - val_accuracy: 0.7013\n",
      "Epoch 127/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.4713 - accuracy: 0.7866 - val_loss: 0.5566 - val_accuracy: 0.7273\n",
      "Epoch 128/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4813 - accuracy: 0.7573 - val_loss: 0.5732 - val_accuracy: 0.7273\n",
      "Epoch 129/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4697 - accuracy: 0.7883 - val_loss: 0.6298 - val_accuracy: 0.6623\n",
      "Epoch 130/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.4891 - accuracy: 0.7655 - val_loss: 0.5559 - val_accuracy: 0.7078\n",
      "Epoch 131/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.4763 - accuracy: 0.7720 - val_loss: 0.5686 - val_accuracy: 0.7078\n",
      "Epoch 132/150\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.4800 - accuracy: 0.7622 - val_loss: 0.5551 - val_accuracy: 0.7143\n",
      "Epoch 133/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.4789 - accuracy: 0.7590 - val_loss: 0.5770 - val_accuracy: 0.6883\n",
      "Epoch 134/150\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 0.4895 - accuracy: 0.7736 - val_loss: 0.5835 - val_accuracy: 0.7013\n",
      "Epoch 135/150\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 0.4687 - accuracy: 0.7752 - val_loss: 0.5575 - val_accuracy: 0.7208\n",
      "Epoch 136/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4876 - accuracy: 0.7606 - val_loss: 0.6071 - val_accuracy: 0.6883\n",
      "Epoch 137/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4768 - accuracy: 0.7866 - val_loss: 0.5627 - val_accuracy: 0.7143\n",
      "Epoch 138/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.4736 - accuracy: 0.7573 - val_loss: 0.5770 - val_accuracy: 0.6948\n",
      "Epoch 139/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4638 - accuracy: 0.7720 - val_loss: 0.5564 - val_accuracy: 0.7403\n",
      "Epoch 140/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4706 - accuracy: 0.7606 - val_loss: 0.5547 - val_accuracy: 0.7338\n",
      "Epoch 141/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4672 - accuracy: 0.7720 - val_loss: 0.5590 - val_accuracy: 0.6948\n",
      "Epoch 142/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4772 - accuracy: 0.7785 - val_loss: 0.5606 - val_accuracy: 0.7208\n",
      "Epoch 143/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4787 - accuracy: 0.7850 - val_loss: 0.5566 - val_accuracy: 0.7143\n",
      "Epoch 144/150\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4628 - accuracy: 0.7850 - val_loss: 0.5740 - val_accuracy: 0.6948\n",
      "Epoch 145/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.4714 - accuracy: 0.7736 - val_loss: 0.5521 - val_accuracy: 0.7078\n",
      "Epoch 146/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.4698 - accuracy: 0.7638 - val_loss: 0.5742 - val_accuracy: 0.7208\n",
      "Epoch 147/150\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.4651 - accuracy: 0.7687 - val_loss: 0.5693 - val_accuracy: 0.7208\n",
      "Epoch 148/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.4675 - accuracy: 0.7818 - val_loss: 0.5503 - val_accuracy: 0.7013\n",
      "Epoch 149/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.4792 - accuracy: 0.7720 - val_loss: 0.5520 - val_accuracy: 0.7143\n",
      "Epoch 150/150\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.4736 - accuracy: 0.7655 - val_loss: 0.5432 - val_accuracy: 0.7078\n",
      "time 47.220261096954346\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4500 - accuracy: 0.7883\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5432 - accuracy: 0.7078\n",
      "train_accuracy : 0.79\n",
      "val_accuracy : 0.71\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "model = my_model((8,))\n",
    "model.compile(loss='binary_crossentropy', optimizer=tf.optimizers.Adam(learning_rate=1e-3), metrics=['accuracy'])\n",
    "tic = time.time()\n",
    "history = model.fit(x_train, y_train,validation_data=(x_val,y_val),epochs=150, batch_size=10,verbose=1)\n",
    "tac = time.time()\n",
    "print(f\"time {tac-tic}\")\n",
    "# _, accuracy = model.evaluate(X, y,verbose=1)\n",
    "_, train_accuracy = model.evaluate(x_train, y_train,verbose=1)\n",
    "_, val_accuracy = model.evaluate(x_val, y_val,verbose=1)\n",
    "print(\"train_accuracy : {:.2f}\".format(train_accuracy))\n",
    "print(\"val_accuracy : {:.2f}\".format(val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question : tracer ces courbes pour le MLP à deux couches cachées (32,16) et une taille de batch de 1. Que constatez vous ?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fbea950b040>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['loss'], label = 'loss')\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "plt.plot(history.history['val_loss'], label = 'val loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy/Loss')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cas standard d'entraînement avec léger overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question : analyser l'accuracy pour plusieurs valeurs de learning_rate et pour plusieurs algo d'apprentissage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:15<00:00, 25.02s/it]\n",
      "100%|██████████| 3/3 [01:11<00:00, 23.84s/it]\n",
      "100%|██████████| 3/3 [01:00<00:00, 20.33s/it]\n",
      "100%|██████████| 3/3 [01:12<00:00, 24.18s/it]\n",
      "100%|██████████| 3/3 [01:02<00:00, 20.97s/it]\n",
      "100%|██████████| 5/5 [05:43<00:00, 68.61s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "optims = [tf.keras.optimizers.SGD, tf.keras.optimizers.RMSprop, tf.keras.optimizers.Adam, tf.keras.optimizers.AdamW, tf.keras.optimizers.Adagrad]\n",
    "lrs = [1e-1,1e-4,1e-6]\n",
    "histos = []\n",
    "accs = []\n",
    "\n",
    "for opt in tqdm(optims, position=0):\n",
    "    hist_lr = []\n",
    "    acc_lr = []\n",
    "    for lr in tqdm(lrs, position=1):\n",
    "        model = my_model((8,))\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt(learning_rate=lr), metrics=['accuracy'])\n",
    "        tic = time.time()\n",
    "        history = model.fit(x_train, y_train,validation_data=(x_val,y_val),epochs=150, batch_size=10,verbose=0)\n",
    "        tac = time.time()\n",
    "        # print(f\"time {tac-tic}\")\n",
    "        _, train_accuracy = model.evaluate(x_train, y_train,verbose=0)\n",
    "        _, val_accuracy = model.evaluate(x_val, y_val,verbose=0)\n",
    "        acc_lr.append([train_accuracy,val_accuracy])\n",
    "        hist_lr.append(history)\n",
    "    \n",
    "    histos.append(hist_lr)\n",
    "    accs.append(acc_lr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"temp/regressionML.keras\"\n",
    "model.save(model_path)\n",
    "restored_model = tf.keras.models.load_model(model_path)\n",
    "restored_model.compile(loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/20 [>.............................] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 645us/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_train)\n",
    "restored_pred = restored_model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "classe 1 and classe 1\n"
     ]
    }
   ],
   "source": [
    "X_1=np.array([5.8,2.6,4.0,1.2,2.4,1.8,6.2,0.1]).reshape((1,-1))\n",
    "X_2=np.array([6.3,3.3,6.0,2.5,2.4,1.8,6.2,0.1]).reshape((1,-1))\n",
    "res1=model.predict([X_1])\n",
    "res2=model.predict([X_2])\n",
    "print('classe {0} and classe {1}'.format(round(res1[0,0]),round(res2[0,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.32289, saving model to ./temp/weights-cb.hdf5\n",
      "50/50 - 0s - loss: 0.4232 - accuracy: 0.7902 - val_loss: 0.3229 - val_accuracy: 0.8699 - 282ms/epoch - 6ms/step\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4153 - accuracy: 0.7963 - val_loss: 0.3783 - val_accuracy: 0.7805 - 162ms/epoch - 3ms/step\n",
      "Epoch 3/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/menierc/PhD/miniconda3/envs/tensorflow/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4355 - accuracy: 0.7963 - val_loss: 0.4107 - val_accuracy: 0.8211 - 158ms/epoch - 3ms/step\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4716 - accuracy: 0.7576 - val_loss: 0.3531 - val_accuracy: 0.8537 - 149ms/epoch - 3ms/step\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4212 - accuracy: 0.8024 - val_loss: 0.4391 - val_accuracy: 0.7886 - 385ms/epoch - 8ms/step\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4344 - accuracy: 0.8024 - val_loss: 0.3320 - val_accuracy: 0.8699 - 172ms/epoch - 3ms/step\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4189 - accuracy: 0.8024 - val_loss: 0.3353 - val_accuracy: 0.8293 - 184ms/epoch - 4ms/step\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4197 - accuracy: 0.7984 - val_loss: 0.3401 - val_accuracy: 0.8618 - 180ms/epoch - 4ms/step\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4291 - accuracy: 0.7882 - val_loss: 0.3404 - val_accuracy: 0.8618 - 177ms/epoch - 4ms/step\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4214 - accuracy: 0.7943 - val_loss: 0.3723 - val_accuracy: 0.8211 - 236ms/epoch - 5ms/step\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4121 - accuracy: 0.8065 - val_loss: 0.3951 - val_accuracy: 0.7886 - 463ms/epoch - 9ms/step\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4117 - accuracy: 0.8086 - val_loss: 0.3468 - val_accuracy: 0.8699 - 198ms/epoch - 4ms/step\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4200 - accuracy: 0.8045 - val_loss: 0.3442 - val_accuracy: 0.8455 - 165ms/epoch - 3ms/step\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4171 - accuracy: 0.7984 - val_loss: 0.4113 - val_accuracy: 0.8130 - 159ms/epoch - 3ms/step\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4281 - accuracy: 0.8147 - val_loss: 0.4016 - val_accuracy: 0.8130 - 192ms/epoch - 4ms/step\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4704 - accuracy: 0.7800 - val_loss: 0.3868 - val_accuracy: 0.8537 - 207ms/epoch - 4ms/step\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4225 - accuracy: 0.7841 - val_loss: 0.3736 - val_accuracy: 0.8293 - 216ms/epoch - 4ms/step\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4266 - accuracy: 0.7800 - val_loss: 0.3703 - val_accuracy: 0.8293 - 213ms/epoch - 4ms/step\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4110 - accuracy: 0.8167 - val_loss: 0.3988 - val_accuracy: 0.8374 - 166ms/epoch - 3ms/step\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4260 - accuracy: 0.7882 - val_loss: 0.3344 - val_accuracy: 0.8699 - 127ms/epoch - 3ms/step\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4063 - accuracy: 0.8004 - val_loss: 0.3714 - val_accuracy: 0.8374 - 136ms/epoch - 3ms/step\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4556 - accuracy: 0.7800 - val_loss: 0.3792 - val_accuracy: 0.8293 - 130ms/epoch - 3ms/step\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4146 - accuracy: 0.8065 - val_loss: 0.3790 - val_accuracy: 0.8293 - 155ms/epoch - 3ms/step\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4171 - accuracy: 0.7943 - val_loss: 0.3686 - val_accuracy: 0.8293 - 138ms/epoch - 3ms/step\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4024 - accuracy: 0.7902 - val_loss: 0.3520 - val_accuracy: 0.8537 - 134ms/epoch - 3ms/step\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4046 - accuracy: 0.8065 - val_loss: 0.4068 - val_accuracy: 0.7886 - 164ms/epoch - 3ms/step\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4207 - accuracy: 0.8004 - val_loss: 0.3991 - val_accuracy: 0.8211 - 161ms/epoch - 3ms/step\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4193 - accuracy: 0.7943 - val_loss: 0.3615 - val_accuracy: 0.8537 - 144ms/epoch - 3ms/step\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4139 - accuracy: 0.7902 - val_loss: 0.3780 - val_accuracy: 0.8211 - 122ms/epoch - 2ms/step\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3933 - accuracy: 0.8147 - val_loss: 0.3562 - val_accuracy: 0.8780 - 134ms/epoch - 3ms/step\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4026 - accuracy: 0.8106 - val_loss: 0.3796 - val_accuracy: 0.8618 - 141ms/epoch - 3ms/step\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3935 - accuracy: 0.8147 - val_loss: 0.3529 - val_accuracy: 0.8537 - 120ms/epoch - 2ms/step\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4229 - accuracy: 0.7821 - val_loss: 0.4561 - val_accuracy: 0.7724 - 126ms/epoch - 3ms/step\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4152 - accuracy: 0.7984 - val_loss: 0.3760 - val_accuracy: 0.8374 - 123ms/epoch - 2ms/step\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4026 - accuracy: 0.8086 - val_loss: 0.3722 - val_accuracy: 0.8455 - 119ms/epoch - 2ms/step\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4104 - accuracy: 0.8106 - val_loss: 0.3591 - val_accuracy: 0.8537 - 128ms/epoch - 3ms/step\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4013 - accuracy: 0.8228 - val_loss: 0.4565 - val_accuracy: 0.7886 - 120ms/epoch - 2ms/step\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4191 - accuracy: 0.8045 - val_loss: 0.4217 - val_accuracy: 0.7805 - 114ms/epoch - 2ms/step\n",
      "Epoch 39/200\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4337 - accuracy: 0.7882 - val_loss: 0.3681 - val_accuracy: 0.8374 - 128ms/epoch - 3ms/step\n",
      "Epoch 40/200\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4190 - accuracy: 0.7699 - val_loss: 0.4452 - val_accuracy: 0.7967 - 123ms/epoch - 2ms/step\n",
      "Epoch 41/200\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4004 - accuracy: 0.8106 - val_loss: 0.4238 - val_accuracy: 0.7967 - 119ms/epoch - 2ms/step\n",
      "Epoch 42/200\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4069 - accuracy: 0.8065 - val_loss: 0.4179 - val_accuracy: 0.7967 - 117ms/epoch - 2ms/step\n",
      "Epoch 43/200\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3994 - accuracy: 0.8269 - val_loss: 0.3733 - val_accuracy: 0.8537 - 121ms/epoch - 2ms/step\n",
      "Epoch 44/200\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4309 - accuracy: 0.7923 - val_loss: 0.4327 - val_accuracy: 0.7886 - 121ms/epoch - 2ms/step\n",
      "Epoch 45/200\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4296 - accuracy: 0.8065 - val_loss: 0.3655 - val_accuracy: 0.8618 - 130ms/epoch - 3ms/step\n",
      "Epoch 46/200\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4025 - accuracy: 0.8065 - val_loss: 0.3616 - val_accuracy: 0.8374 - 126ms/epoch - 3ms/step\n",
      "Epoch 47/200\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4057 - accuracy: 0.8269 - val_loss: 0.3863 - val_accuracy: 0.8130 - 115ms/epoch - 2ms/step\n",
      "Epoch 48/200\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4095 - accuracy: 0.7902 - val_loss: 0.3927 - val_accuracy: 0.8130 - 116ms/epoch - 2ms/step\n",
      "Epoch 49/200\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3922 - accuracy: 0.8106 - val_loss: 0.4012 - val_accuracy: 0.8130 - 128ms/epoch - 3ms/step\n",
      "Epoch 50/200\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4106 - accuracy: 0.8228 - val_loss: 0.3844 - val_accuracy: 0.8293 - 133ms/epoch - 3ms/step\n",
      "Epoch 51/200\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4120 - accuracy: 0.8106 - val_loss: 0.3807 - val_accuracy: 0.8293 - 124ms/epoch - 2ms/step\n",
      "Epoch 52/200\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3912 - accuracy: 0.8106 - val_loss: 0.3735 - val_accuracy: 0.8537 - 120ms/epoch - 2ms/step\n",
      "Epoch 53/200\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4006 - accuracy: 0.8106 - val_loss: 0.4023 - val_accuracy: 0.8049 - 125ms/epoch - 3ms/step\n",
      "Epoch 54/200\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3889 - accuracy: 0.8004 - val_loss: 0.3893 - val_accuracy: 0.8130 - 120ms/epoch - 2ms/step\n",
      "Epoch 55/200\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4098 - accuracy: 0.7943 - val_loss: 0.4238 - val_accuracy: 0.8049 - 124ms/epoch - 2ms/step\n",
      "Epoch 56/200\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3956 - accuracy: 0.8106 - val_loss: 0.3588 - val_accuracy: 0.8455 - 115ms/epoch - 2ms/step\n",
      "Epoch 57/200\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3967 - accuracy: 0.8086 - val_loss: 0.3711 - val_accuracy: 0.8374 - 119ms/epoch - 2ms/step\n",
      "Epoch 58/200\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3911 - accuracy: 0.8269 - val_loss: 0.3912 - val_accuracy: 0.8455 - 121ms/epoch - 2ms/step\n",
      "Epoch 59/200\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4204 - accuracy: 0.7902 - val_loss: 0.3922 - val_accuracy: 0.8293 - 115ms/epoch - 2ms/step\n",
      "Epoch 60/200\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3881 - accuracy: 0.8289 - val_loss: 0.3685 - val_accuracy: 0.8374 - 131ms/epoch - 3ms/step\n",
      "Epoch 61/200\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3948 - accuracy: 0.8024 - val_loss: 0.3830 - val_accuracy: 0.8293 - 115ms/epoch - 2ms/step\n",
      "Epoch 62/200\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3922 - accuracy: 0.8045 - val_loss: 0.3814 - val_accuracy: 0.8211 - 119ms/epoch - 2ms/step\n",
      "Epoch 63/200\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4251 - accuracy: 0.8024 - val_loss: 0.4011 - val_accuracy: 0.8374 - 128ms/epoch - 3ms/step\n",
      "Epoch 64/200\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4154 - accuracy: 0.7902 - val_loss: 0.3716 - val_accuracy: 0.8374 - 118ms/epoch - 2ms/step\n",
      "Epoch 65/200\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3878 - accuracy: 0.8126 - val_loss: 0.3790 - val_accuracy: 0.8293 - 114ms/epoch - 2ms/step\n",
      "Epoch 66/200\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3829 - accuracy: 0.8086 - val_loss: 0.3975 - val_accuracy: 0.8211 - 130ms/epoch - 3ms/step\n",
      "Epoch 67/200\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3966 - accuracy: 0.8086 - val_loss: 0.3619 - val_accuracy: 0.8537 - 127ms/epoch - 3ms/step\n",
      "Epoch 68/200\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3772 - accuracy: 0.8228 - val_loss: 0.3853 - val_accuracy: 0.8455 - 134ms/epoch - 3ms/step\n",
      "Epoch 69/200\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3926 - accuracy: 0.8310 - val_loss: 0.3645 - val_accuracy: 0.8455 - 174ms/epoch - 3ms/step\n",
      "Epoch 70/200\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3827 - accuracy: 0.8147 - val_loss: 0.3985 - val_accuracy: 0.8049 - 123ms/epoch - 2ms/step\n",
      "Epoch 71/200\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3887 - accuracy: 0.8187 - val_loss: 0.3868 - val_accuracy: 0.8130 - 131ms/epoch - 3ms/step\n",
      "Epoch 72/200\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4031 - accuracy: 0.8167 - val_loss: 0.4269 - val_accuracy: 0.8049 - 140ms/epoch - 3ms/step\n",
      "Epoch 73/200\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3950 - accuracy: 0.8126 - val_loss: 0.3944 - val_accuracy: 0.8211 - 134ms/epoch - 3ms/step\n",
      "Epoch 74/200\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3767 - accuracy: 0.8208 - val_loss: 0.4077 - val_accuracy: 0.8130 - 165ms/epoch - 3ms/step\n",
      "Epoch 75/200\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3719 - accuracy: 0.8289 - val_loss: 0.3853 - val_accuracy: 0.8293 - 149ms/epoch - 3ms/step\n",
      "Epoch 76/200\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4162 - accuracy: 0.8004 - val_loss: 0.4112 - val_accuracy: 0.8049 - 160ms/epoch - 3ms/step\n",
      "Epoch 77/200\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3937 - accuracy: 0.8228 - val_loss: 0.4115 - val_accuracy: 0.7967 - 188ms/epoch - 4ms/step\n",
      "Epoch 78/200\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3909 - accuracy: 0.8187 - val_loss: 0.4430 - val_accuracy: 0.7886 - 184ms/epoch - 4ms/step\n",
      "Epoch 79/200\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4003 - accuracy: 0.8106 - val_loss: 0.3855 - val_accuracy: 0.8537 - 186ms/epoch - 4ms/step\n",
      "Epoch 80/200\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3815 - accuracy: 0.8208 - val_loss: 0.3771 - val_accuracy: 0.8537 - 156ms/epoch - 3ms/step\n",
      "Epoch 81/200\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3926 - accuracy: 0.7984 - val_loss: 0.3862 - val_accuracy: 0.7886 - 157ms/epoch - 3ms/step\n",
      "Epoch 82/200\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3947 - accuracy: 0.8024 - val_loss: 0.4841 - val_accuracy: 0.7967 - 148ms/epoch - 3ms/step\n",
      "Epoch 83/200\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4145 - accuracy: 0.8045 - val_loss: 0.3909 - val_accuracy: 0.8293 - 165ms/epoch - 3ms/step\n",
      "Epoch 84/200\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3698 - accuracy: 0.8248 - val_loss: 0.4100 - val_accuracy: 0.8293 - 142ms/epoch - 3ms/step\n",
      "Epoch 85/200\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3826 - accuracy: 0.8167 - val_loss: 0.3923 - val_accuracy: 0.8130 - 156ms/epoch - 3ms/step\n",
      "Epoch 86/200\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3933 - accuracy: 0.8187 - val_loss: 0.4177 - val_accuracy: 0.7967 - 149ms/epoch - 3ms/step\n",
      "Epoch 87/200\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3926 - accuracy: 0.8167 - val_loss: 0.4234 - val_accuracy: 0.7805 - 163ms/epoch - 3ms/step\n",
      "Epoch 88/200\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3926 - accuracy: 0.8024 - val_loss: 0.3829 - val_accuracy: 0.8455 - 167ms/epoch - 3ms/step\n",
      "Epoch 89/200\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3756 - accuracy: 0.8147 - val_loss: 0.3801 - val_accuracy: 0.8374 - 143ms/epoch - 3ms/step\n",
      "Epoch 90/200\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4201 - accuracy: 0.8065 - val_loss: 0.3641 - val_accuracy: 0.8374 - 136ms/epoch - 3ms/step\n",
      "Epoch 91/200\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3674 - accuracy: 0.8269 - val_loss: 0.4292 - val_accuracy: 0.7886 - 140ms/epoch - 3ms/step\n",
      "Epoch 92/200\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4003 - accuracy: 0.8086 - val_loss: 0.4107 - val_accuracy: 0.7967 - 145ms/epoch - 3ms/step\n",
      "Epoch 93/200\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3970 - accuracy: 0.8147 - val_loss: 0.3835 - val_accuracy: 0.8537 - 150ms/epoch - 3ms/step\n",
      "Epoch 94/200\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3742 - accuracy: 0.8228 - val_loss: 0.4389 - val_accuracy: 0.7642 - 139ms/epoch - 3ms/step\n",
      "Epoch 95/200\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3877 - accuracy: 0.8086 - val_loss: 0.4148 - val_accuracy: 0.8049 - 134ms/epoch - 3ms/step\n",
      "Epoch 96/200\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4000 - accuracy: 0.8086 - val_loss: 0.3861 - val_accuracy: 0.8211 - 141ms/epoch - 3ms/step\n",
      "Epoch 97/200\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3746 - accuracy: 0.8045 - val_loss: 0.4031 - val_accuracy: 0.8130 - 134ms/epoch - 3ms/step\n",
      "Epoch 98/200\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3984 - accuracy: 0.8126 - val_loss: 0.4345 - val_accuracy: 0.7967 - 136ms/epoch - 3ms/step\n",
      "Epoch 99/200\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3752 - accuracy: 0.8248 - val_loss: 0.4052 - val_accuracy: 0.7967 - 262ms/epoch - 5ms/step\n",
      "Epoch 100/200\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3792 - accuracy: 0.8228 - val_loss: 0.4205 - val_accuracy: 0.7967 - 162ms/epoch - 3ms/step\n",
      "Epoch 101/200\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3721 - accuracy: 0.8208 - val_loss: 0.3897 - val_accuracy: 0.8374 - 135ms/epoch - 3ms/step\n",
      "Epoch 102/200\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3741 - accuracy: 0.8289 - val_loss: 0.3914 - val_accuracy: 0.8211 - 130ms/epoch - 3ms/step\n",
      "Epoch 103/200\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3747 - accuracy: 0.8147 - val_loss: 0.4033 - val_accuracy: 0.8049 - 137ms/epoch - 3ms/step\n",
      "Epoch 104/200\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3884 - accuracy: 0.8106 - val_loss: 0.5181 - val_accuracy: 0.7154 - 151ms/epoch - 3ms/step\n",
      "Epoch 105/200\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3700 - accuracy: 0.8330 - val_loss: 0.3750 - val_accuracy: 0.8293 - 144ms/epoch - 3ms/step\n",
      "Epoch 106/200\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3613 - accuracy: 0.8269 - val_loss: 0.3779 - val_accuracy: 0.8455 - 155ms/epoch - 3ms/step\n",
      "Epoch 107/200\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3596 - accuracy: 0.8187 - val_loss: 0.4133 - val_accuracy: 0.8293 - 134ms/epoch - 3ms/step\n",
      "Epoch 108/200\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3781 - accuracy: 0.8147 - val_loss: 0.3858 - val_accuracy: 0.8618 - 154ms/epoch - 3ms/step\n",
      "Epoch 109/200\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3778 - accuracy: 0.8208 - val_loss: 0.3820 - val_accuracy: 0.8374 - 138ms/epoch - 3ms/step\n",
      "Epoch 110/200\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3668 - accuracy: 0.8269 - val_loss: 0.3951 - val_accuracy: 0.8130 - 128ms/epoch - 3ms/step\n",
      "Epoch 111/200\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3780 - accuracy: 0.8269 - val_loss: 0.5006 - val_accuracy: 0.8049 - 136ms/epoch - 3ms/step\n",
      "Epoch 112/200\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3902 - accuracy: 0.8086 - val_loss: 0.3912 - val_accuracy: 0.8455 - 139ms/epoch - 3ms/step\n",
      "Epoch 113/200\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3759 - accuracy: 0.8350 - val_loss: 0.3924 - val_accuracy: 0.8293 - 129ms/epoch - 3ms/step\n",
      "Epoch 114/200\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3754 - accuracy: 0.8106 - val_loss: 0.3842 - val_accuracy: 0.8293 - 141ms/epoch - 3ms/step\n",
      "Epoch 115/200\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3589 - accuracy: 0.8432 - val_loss: 0.3905 - val_accuracy: 0.8049 - 135ms/epoch - 3ms/step\n",
      "Epoch 116/200\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3636 - accuracy: 0.8330 - val_loss: 0.3765 - val_accuracy: 0.8374 - 144ms/epoch - 3ms/step\n",
      "Epoch 117/200\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3681 - accuracy: 0.8391 - val_loss: 0.3988 - val_accuracy: 0.8130 - 151ms/epoch - 3ms/step\n",
      "Epoch 118/200\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3732 - accuracy: 0.8248 - val_loss: 0.4771 - val_accuracy: 0.7805 - 153ms/epoch - 3ms/step\n",
      "Epoch 119/200\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4725 - accuracy: 0.7902 - val_loss: 0.4231 - val_accuracy: 0.7967 - 127ms/epoch - 3ms/step\n",
      "Epoch 120/200\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4045 - accuracy: 0.8126 - val_loss: 0.4305 - val_accuracy: 0.7967 - 133ms/epoch - 3ms/step\n",
      "Epoch 121/200\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3681 - accuracy: 0.8330 - val_loss: 0.3848 - val_accuracy: 0.8130 - 138ms/epoch - 3ms/step\n",
      "Epoch 122/200\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3665 - accuracy: 0.8228 - val_loss: 0.4386 - val_accuracy: 0.7886 - 134ms/epoch - 3ms/step\n",
      "Epoch 123/200\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3670 - accuracy: 0.8147 - val_loss: 0.4101 - val_accuracy: 0.8374 - 138ms/epoch - 3ms/step\n",
      "Epoch 124/200\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3864 - accuracy: 0.8045 - val_loss: 0.3837 - val_accuracy: 0.8293 - 129ms/epoch - 3ms/step\n",
      "Epoch 125/200\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3796 - accuracy: 0.8167 - val_loss: 0.3891 - val_accuracy: 0.8374 - 156ms/epoch - 3ms/step\n",
      "Epoch 126/200\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3589 - accuracy: 0.8310 - val_loss: 0.3775 - val_accuracy: 0.8049 - 120ms/epoch - 2ms/step\n",
      "Epoch 127/200\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3992 - accuracy: 0.8004 - val_loss: 0.4200 - val_accuracy: 0.8211 - 138ms/epoch - 3ms/step\n",
      "Epoch 128/200\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3870 - accuracy: 0.8106 - val_loss: 0.4697 - val_accuracy: 0.7805 - 130ms/epoch - 3ms/step\n",
      "Epoch 129/200\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3842 - accuracy: 0.8045 - val_loss: 0.4517 - val_accuracy: 0.8049 - 124ms/epoch - 2ms/step\n",
      "Epoch 130/200\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3681 - accuracy: 0.8310 - val_loss: 0.3988 - val_accuracy: 0.7967 - 133ms/epoch - 3ms/step\n",
      "Epoch 131/200\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3631 - accuracy: 0.8289 - val_loss: 0.4076 - val_accuracy: 0.7967 - 142ms/epoch - 3ms/step\n",
      "Epoch 132/200\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3858 - accuracy: 0.8106 - val_loss: 0.4307 - val_accuracy: 0.8374 - 138ms/epoch - 3ms/step\n",
      "Epoch 133/200\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4182 - accuracy: 0.8065 - val_loss: 0.4094 - val_accuracy: 0.8374 - 128ms/epoch - 3ms/step\n",
      "Epoch 134/200\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3581 - accuracy: 0.8248 - val_loss: 0.3886 - val_accuracy: 0.8618 - 139ms/epoch - 3ms/step\n",
      "Epoch 135/200\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3878 - accuracy: 0.8269 - val_loss: 0.3959 - val_accuracy: 0.8130 - 437ms/epoch - 9ms/step\n",
      "Epoch 136/200\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3752 - accuracy: 0.8147 - val_loss: 0.4571 - val_accuracy: 0.8130 - 251ms/epoch - 5ms/step\n",
      "Epoch 137/200\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4624 - accuracy: 0.8106 - val_loss: 0.3771 - val_accuracy: 0.8780 - 176ms/epoch - 4ms/step\n",
      "Epoch 138/200\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3611 - accuracy: 0.8350 - val_loss: 0.4058 - val_accuracy: 0.8211 - 130ms/epoch - 3ms/step\n",
      "Epoch 139/200\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3633 - accuracy: 0.8350 - val_loss: 0.3879 - val_accuracy: 0.8130 - 128ms/epoch - 3ms/step\n",
      "Epoch 140/200\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3500 - accuracy: 0.8350 - val_loss: 0.4038 - val_accuracy: 0.8049 - 111ms/epoch - 2ms/step\n",
      "Epoch 141/200\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3727 - accuracy: 0.8167 - val_loss: 0.3711 - val_accuracy: 0.8374 - 112ms/epoch - 2ms/step\n",
      "Epoch 142/200\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3653 - accuracy: 0.8187 - val_loss: 0.4117 - val_accuracy: 0.8130 - 116ms/epoch - 2ms/step\n",
      "Epoch 143/200\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3613 - accuracy: 0.8411 - val_loss: 0.3727 - val_accuracy: 0.8455 - 123ms/epoch - 2ms/step\n",
      "Epoch 144/200\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3495 - accuracy: 0.8350 - val_loss: 0.4308 - val_accuracy: 0.8211 - 133ms/epoch - 3ms/step\n",
      "Epoch 145/200\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3812 - accuracy: 0.8310 - val_loss: 0.4009 - val_accuracy: 0.8130 - 116ms/epoch - 2ms/step\n",
      "Epoch 146/200\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3554 - accuracy: 0.8248 - val_loss: 0.4011 - val_accuracy: 0.8537 - 128ms/epoch - 3ms/step\n",
      "Epoch 147/200\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3700 - accuracy: 0.8248 - val_loss: 0.4208 - val_accuracy: 0.8130 - 131ms/epoch - 3ms/step\n",
      "Epoch 148/200\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3532 - accuracy: 0.8350 - val_loss: 0.4065 - val_accuracy: 0.8211 - 193ms/epoch - 4ms/step\n",
      "Epoch 149/200\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3470 - accuracy: 0.8411 - val_loss: 0.4424 - val_accuracy: 0.7805 - 153ms/epoch - 3ms/step\n",
      "Epoch 150/200\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3750 - accuracy: 0.8126 - val_loss: 0.4376 - val_accuracy: 0.7805 - 146ms/epoch - 3ms/step\n",
      "Epoch 151/200\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3656 - accuracy: 0.8432 - val_loss: 0.5215 - val_accuracy: 0.8049 - 152ms/epoch - 3ms/step\n",
      "Epoch 152/200\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3612 - accuracy: 0.8289 - val_loss: 0.4311 - val_accuracy: 0.7967 - 220ms/epoch - 4ms/step\n",
      "Epoch 153/200\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3466 - accuracy: 0.8432 - val_loss: 0.4460 - val_accuracy: 0.7886 - 156ms/epoch - 3ms/step\n",
      "Epoch 154/200\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3702 - accuracy: 0.8208 - val_loss: 0.4436 - val_accuracy: 0.8049 - 133ms/epoch - 3ms/step\n",
      "Epoch 155/200\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3893 - accuracy: 0.8086 - val_loss: 0.4260 - val_accuracy: 0.7967 - 149ms/epoch - 3ms/step\n",
      "Epoch 156/200\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3525 - accuracy: 0.8391 - val_loss: 0.4130 - val_accuracy: 0.8211 - 140ms/epoch - 3ms/step\n",
      "Epoch 157/200\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3591 - accuracy: 0.8289 - val_loss: 0.4352 - val_accuracy: 0.7967 - 131ms/epoch - 3ms/step\n",
      "Epoch 158/200\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3695 - accuracy: 0.8248 - val_loss: 0.4274 - val_accuracy: 0.8049 - 122ms/epoch - 2ms/step\n",
      "Epoch 159/200\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3616 - accuracy: 0.8411 - val_loss: 0.4365 - val_accuracy: 0.8130 - 119ms/epoch - 2ms/step\n",
      "Epoch 160/200\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3490 - accuracy: 0.8391 - val_loss: 0.4055 - val_accuracy: 0.8455 - 114ms/epoch - 2ms/step\n",
      "Epoch 161/200\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3531 - accuracy: 0.8310 - val_loss: 0.4356 - val_accuracy: 0.7967 - 122ms/epoch - 2ms/step\n",
      "Epoch 162/200\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4331 - accuracy: 0.7963 - val_loss: 0.4104 - val_accuracy: 0.8537 - 135ms/epoch - 3ms/step\n",
      "Epoch 163/200\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.4091 - accuracy: 0.8208 - val_loss: 0.4465 - val_accuracy: 0.8537 - 136ms/epoch - 3ms/step\n",
      "Epoch 164/200\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3774 - accuracy: 0.8289 - val_loss: 0.4088 - val_accuracy: 0.8455 - 142ms/epoch - 3ms/step\n",
      "Epoch 165/200\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3533 - accuracy: 0.8371 - val_loss: 0.4293 - val_accuracy: 0.8211 - 135ms/epoch - 3ms/step\n",
      "Epoch 166/200\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3498 - accuracy: 0.8371 - val_loss: 0.4435 - val_accuracy: 0.8374 - 145ms/epoch - 3ms/step\n",
      "Epoch 167/200\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3982 - accuracy: 0.8187 - val_loss: 0.4389 - val_accuracy: 0.7805 - 141ms/epoch - 3ms/step\n",
      "Epoch 168/200\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3455 - accuracy: 0.8452 - val_loss: 0.4524 - val_accuracy: 0.8293 - 121ms/epoch - 2ms/step\n",
      "Epoch 169/200\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3431 - accuracy: 0.8493 - val_loss: 0.4619 - val_accuracy: 0.8049 - 142ms/epoch - 3ms/step\n",
      "Epoch 170/200\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3535 - accuracy: 0.8330 - val_loss: 0.4120 - val_accuracy: 0.8211 - 154ms/epoch - 3ms/step\n",
      "Epoch 171/200\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3504 - accuracy: 0.8187 - val_loss: 0.4819 - val_accuracy: 0.7967 - 142ms/epoch - 3ms/step\n",
      "Epoch 172/200\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3496 - accuracy: 0.8330 - val_loss: 0.4498 - val_accuracy: 0.7967 - 158ms/epoch - 3ms/step\n",
      "Epoch 173/200\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3843 - accuracy: 0.8004 - val_loss: 0.4561 - val_accuracy: 0.8130 - 138ms/epoch - 3ms/step\n",
      "Epoch 174/200\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3441 - accuracy: 0.8432 - val_loss: 0.4359 - val_accuracy: 0.8211 - 153ms/epoch - 3ms/step\n",
      "Epoch 175/200\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3604 - accuracy: 0.8330 - val_loss: 0.4210 - val_accuracy: 0.8049 - 138ms/epoch - 3ms/step\n",
      "Epoch 176/200\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3715 - accuracy: 0.8106 - val_loss: 0.4191 - val_accuracy: 0.8130 - 136ms/epoch - 3ms/step\n",
      "Epoch 177/200\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3420 - accuracy: 0.8391 - val_loss: 0.4435 - val_accuracy: 0.8293 - 127ms/epoch - 3ms/step\n",
      "Epoch 178/200\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3554 - accuracy: 0.8330 - val_loss: 0.4474 - val_accuracy: 0.7805 - 126ms/epoch - 3ms/step\n",
      "Epoch 179/200\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3537 - accuracy: 0.8391 - val_loss: 0.4087 - val_accuracy: 0.8455 - 122ms/epoch - 2ms/step\n",
      "Epoch 180/200\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3490 - accuracy: 0.8493 - val_loss: 0.4294 - val_accuracy: 0.8374 - 121ms/epoch - 2ms/step\n",
      "Epoch 181/200\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3532 - accuracy: 0.8350 - val_loss: 0.4174 - val_accuracy: 0.8618 - 146ms/epoch - 3ms/step\n",
      "Epoch 182/200\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3523 - accuracy: 0.8310 - val_loss: 0.4295 - val_accuracy: 0.8374 - 150ms/epoch - 3ms/step\n",
      "Epoch 183/200\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3319 - accuracy: 0.8513 - val_loss: 0.4286 - val_accuracy: 0.8374 - 123ms/epoch - 2ms/step\n",
      "Epoch 184/200\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3363 - accuracy: 0.8473 - val_loss: 0.4414 - val_accuracy: 0.8293 - 124ms/epoch - 2ms/step\n",
      "Epoch 185/200\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3441 - accuracy: 0.8432 - val_loss: 0.4484 - val_accuracy: 0.8374 - 126ms/epoch - 3ms/step\n",
      "Epoch 186/200\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3349 - accuracy: 0.8432 - val_loss: 0.4234 - val_accuracy: 0.8374 - 128ms/epoch - 3ms/step\n",
      "Epoch 187/200\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3343 - accuracy: 0.8411 - val_loss: 0.4262 - val_accuracy: 0.8537 - 131ms/epoch - 3ms/step\n",
      "Epoch 188/200\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3361 - accuracy: 0.8513 - val_loss: 0.4301 - val_accuracy: 0.8374 - 122ms/epoch - 2ms/step\n",
      "Epoch 189/200\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3892 - accuracy: 0.8330 - val_loss: 0.4455 - val_accuracy: 0.8211 - 120ms/epoch - 2ms/step\n",
      "Epoch 190/200\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3323 - accuracy: 0.8411 - val_loss: 0.4418 - val_accuracy: 0.8049 - 125ms/epoch - 3ms/step\n",
      "Epoch 191/200\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3529 - accuracy: 0.8248 - val_loss: 0.4353 - val_accuracy: 0.7967 - 133ms/epoch - 3ms/step\n",
      "Epoch 192/200\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3469 - accuracy: 0.8350 - val_loss: 0.4543 - val_accuracy: 0.8211 - 128ms/epoch - 3ms/step\n",
      "Epoch 193/200\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3356 - accuracy: 0.8473 - val_loss: 0.4728 - val_accuracy: 0.7886 - 144ms/epoch - 3ms/step\n",
      "Epoch 194/200\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3516 - accuracy: 0.8411 - val_loss: 0.4420 - val_accuracy: 0.8699 - 152ms/epoch - 3ms/step\n",
      "Epoch 195/200\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3472 - accuracy: 0.8493 - val_loss: 0.4671 - val_accuracy: 0.8130 - 119ms/epoch - 2ms/step\n",
      "Epoch 196/200\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3445 - accuracy: 0.8432 - val_loss: 0.4274 - val_accuracy: 0.8049 - 116ms/epoch - 2ms/step\n",
      "Epoch 197/200\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3332 - accuracy: 0.8554 - val_loss: 0.4243 - val_accuracy: 0.8049 - 131ms/epoch - 3ms/step\n",
      "Epoch 198/200\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3458 - accuracy: 0.8432 - val_loss: 0.4150 - val_accuracy: 0.8293 - 138ms/epoch - 3ms/step\n",
      "Epoch 199/200\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3347 - accuracy: 0.8656 - val_loss: 0.4514 - val_accuracy: 0.7724 - 130ms/epoch - 3ms/step\n",
      "Epoch 200/200\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.32289\n",
      "50/50 - 0s - loss: 0.3461 - accuracy: 0.8330 - val_loss: 0.4665 - val_accuracy: 0.8130 - 153ms/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=\"./temp/weights-{epoch:03d}-{val_loss:.4f}.hdf5\", monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=\"./temp/weights-cb.hdf5\", monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1)\n",
    "callbacks = [checkpoint]\n",
    "history=model.fit(x_train, y_train, epochs=200, batch_size=10, shuffle=True, validation_split=0.2, callbacks=callbacks, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question : une fois l'entraînement terminé, charger le dernier fichier de poids sauvegardé avec la callback et inférer ce réseau sur la base x_train afin d'en calculer la précision\n",
    "comme vous l'avez fait dans le sujet précédent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 923us/step\n"
     ]
    }
   ],
   "source": [
    "restored_model = my_model((8,))\n",
    "restored_model.load_weights(\"./temp/weights-cb.hdf5\")\n",
    "\n",
    "pred = restored_model.predict(x_train)\n",
    "acc = ((pred>0.5).reshape((-1))==y_train).sum()/pred.shape[0]\n",
    "print(f\"accuracy : {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.6889 - accuracy: 0.5760\n",
      "Epoch 1: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 1s 6ms/step - loss: 0.6650 - accuracy: 0.6091 - val_loss: 0.6092 - val_accuracy: 0.6494\n",
      "Epoch 2/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.5581 - accuracy: 0.7321\n",
      "Epoch 2: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.5465 - accuracy: 0.7378 - val_loss: 0.5581 - val_accuracy: 0.7143\n",
      "Epoch 3/350\n",
      "58/62 [===========================>..] - ETA: 0s - loss: 0.5110 - accuracy: 0.7534\n",
      "Epoch 3: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5040 - accuracy: 0.7541 - val_loss: 0.5288 - val_accuracy: 0.7597\n",
      "Epoch 4/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.4978 - accuracy: 0.7545\n",
      "Epoch 4: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4818 - accuracy: 0.7638 - val_loss: 0.5151 - val_accuracy: 0.7597\n",
      "Epoch 5/350\n",
      "37/62 [================>.............] - ETA: 0s - loss: 0.5022 - accuracy: 0.7568\n",
      "Epoch 5: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4701 - accuracy: 0.7818 - val_loss: 0.5089 - val_accuracy: 0.7597\n",
      "Epoch 6/350\n",
      "36/62 [================>.............] - ETA: 0s - loss: 0.4967 - accuracy: 0.7583\n",
      "Epoch 6: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.4632 - accuracy: 0.7834 - val_loss: 0.5059 - val_accuracy: 0.7662\n",
      "Epoch 7/350\n",
      "38/62 [=================>............] - ETA: 0s - loss: 0.4866 - accuracy: 0.7632\n",
      "Epoch 7: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4586 - accuracy: 0.7834 - val_loss: 0.5042 - val_accuracy: 0.7662\n",
      "Epoch 8/350\n",
      "57/62 [==========================>...] - ETA: 0s - loss: 0.4695 - accuracy: 0.7737\n",
      "Epoch 8: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4552 - accuracy: 0.7883 - val_loss: 0.5033 - val_accuracy: 0.7662\n",
      "Epoch 9/350\n",
      "34/62 [===============>..............] - ETA: 0s - loss: 0.4923 - accuracy: 0.7529\n",
      "Epoch 9: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4526 - accuracy: 0.7915 - val_loss: 0.5026 - val_accuracy: 0.7597\n",
      "Epoch 10/350\n",
      "40/62 [==================>...........] - ETA: 0s - loss: 0.4734 - accuracy: 0.7650\n",
      "Epoch 10: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4502 - accuracy: 0.7883 - val_loss: 0.5025 - val_accuracy: 0.7662\n",
      "Epoch 11/350\n",
      "33/62 [==============>...............] - ETA: 0s - loss: 0.4884 - accuracy: 0.7515\n",
      "Epoch 11: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4483 - accuracy: 0.7883 - val_loss: 0.5024 - val_accuracy: 0.7662\n",
      "Epoch 12/350\n",
      "33/62 [==============>...............] - ETA: 0s - loss: 0.4869 - accuracy: 0.7515\n",
      "Epoch 12: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4464 - accuracy: 0.7883 - val_loss: 0.5030 - val_accuracy: 0.7662\n",
      "Epoch 13/350\n",
      "38/62 [=================>............] - ETA: 0s - loss: 0.4755 - accuracy: 0.7605\n",
      "Epoch 13: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4447 - accuracy: 0.7883 - val_loss: 0.5031 - val_accuracy: 0.7662\n",
      "Epoch 14/350\n",
      "33/62 [==============>...............] - ETA: 0s - loss: 0.4843 - accuracy: 0.7545\n",
      "Epoch 14: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4430 - accuracy: 0.7915 - val_loss: 0.5033 - val_accuracy: 0.7662\n",
      "Epoch 15/350\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.4492 - accuracy: 0.7847\n",
      "Epoch 15: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4415 - accuracy: 0.7915 - val_loss: 0.5038 - val_accuracy: 0.7597\n",
      "Epoch 16/350\n",
      "40/62 [==================>...........] - ETA: 0s - loss: 0.4644 - accuracy: 0.7650\n",
      "Epoch 16: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4400 - accuracy: 0.7915 - val_loss: 0.5044 - val_accuracy: 0.7597\n",
      "Epoch 17/350\n",
      "48/62 [======================>.......] - ETA: 0s - loss: 0.4610 - accuracy: 0.7688\n",
      "Epoch 17: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4385 - accuracy: 0.7932 - val_loss: 0.5049 - val_accuracy: 0.7597\n",
      "Epoch 18/350\n",
      "39/62 [=================>............] - ETA: 0s - loss: 0.4644 - accuracy: 0.7692\n",
      "Epoch 18: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4370 - accuracy: 0.7964 - val_loss: 0.5052 - val_accuracy: 0.7597\n",
      "Epoch 19/350\n",
      "35/62 [===============>..............] - ETA: 0s - loss: 0.4784 - accuracy: 0.7600\n",
      "Epoch 19: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4355 - accuracy: 0.7964 - val_loss: 0.5061 - val_accuracy: 0.7597\n",
      "Epoch 20/350\n",
      "39/62 [=================>............] - ETA: 0s - loss: 0.4615 - accuracy: 0.7692\n",
      "Epoch 20: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4340 - accuracy: 0.7964 - val_loss: 0.5065 - val_accuracy: 0.7597\n",
      "Epoch 21/350\n",
      "35/62 [===============>..............] - ETA: 0s - loss: 0.4753 - accuracy: 0.7600\n",
      "Epoch 21: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4324 - accuracy: 0.7980 - val_loss: 0.5066 - val_accuracy: 0.7597\n",
      "Epoch 22/350\n",
      "37/62 [================>.............] - ETA: 0s - loss: 0.4695 - accuracy: 0.7676\n",
      "Epoch 22: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4309 - accuracy: 0.7980 - val_loss: 0.5066 - val_accuracy: 0.7597\n",
      "Epoch 23/350\n",
      "47/62 [=====================>........] - ETA: 0s - loss: 0.4509 - accuracy: 0.7745\n",
      "Epoch 23: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4292 - accuracy: 0.7980 - val_loss: 0.5070 - val_accuracy: 0.7662\n",
      "Epoch 24/350\n",
      "51/62 [=======================>......] - ETA: 0s - loss: 0.4506 - accuracy: 0.7804\n",
      "Epoch 24: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4278 - accuracy: 0.8013 - val_loss: 0.5074 - val_accuracy: 0.7662\n",
      "Epoch 25/350\n",
      "51/62 [=======================>......] - ETA: 0s - loss: 0.4488 - accuracy: 0.7784\n",
      "Epoch 25: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4262 - accuracy: 0.7997 - val_loss: 0.5085 - val_accuracy: 0.7662\n",
      "Epoch 26/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.4432 - accuracy: 0.7855\n",
      "Epoch 26: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4248 - accuracy: 0.8013 - val_loss: 0.5090 - val_accuracy: 0.7662\n",
      "Epoch 27/350\n",
      "48/62 [======================>.......] - ETA: 0s - loss: 0.4448 - accuracy: 0.7792\n",
      "Epoch 27: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4233 - accuracy: 0.8013 - val_loss: 0.5091 - val_accuracy: 0.7597\n",
      "Epoch 28/350\n",
      "57/62 [==========================>...] - ETA: 0s - loss: 0.4361 - accuracy: 0.7877\n",
      "Epoch 28: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4219 - accuracy: 0.8013 - val_loss: 0.5100 - val_accuracy: 0.7597\n",
      "Epoch 29/350\n",
      "45/62 [====================>.........] - ETA: 0s - loss: 0.4462 - accuracy: 0.7733\n",
      "Epoch 29: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4206 - accuracy: 0.8013 - val_loss: 0.5101 - val_accuracy: 0.7662\n",
      "Epoch 30/350\n",
      "52/62 [========================>.....] - ETA: 0s - loss: 0.4402 - accuracy: 0.7846\n",
      "Epoch 30: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4191 - accuracy: 0.8029 - val_loss: 0.5107 - val_accuracy: 0.7597\n",
      "Epoch 31/350\n",
      "47/62 [=====================>........] - ETA: 0s - loss: 0.4384 - accuracy: 0.7787\n",
      "Epoch 31: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4177 - accuracy: 0.8013 - val_loss: 0.5112 - val_accuracy: 0.7597\n",
      "Epoch 32/350\n",
      "40/62 [==================>...........] - ETA: 0s - loss: 0.4406 - accuracy: 0.7800\n",
      "Epoch 32: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4164 - accuracy: 0.8013 - val_loss: 0.5125 - val_accuracy: 0.7597\n",
      "Epoch 33/350\n",
      "46/62 [=====================>........] - ETA: 0s - loss: 0.4362 - accuracy: 0.7804\n",
      "Epoch 33: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4150 - accuracy: 0.8029 - val_loss: 0.5133 - val_accuracy: 0.7662\n",
      "Epoch 34/350\n",
      "42/62 [===================>..........] - ETA: 0s - loss: 0.4450 - accuracy: 0.7786\n",
      "Epoch 34: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4135 - accuracy: 0.8046 - val_loss: 0.5145 - val_accuracy: 0.7727\n",
      "Epoch 35/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.4265 - accuracy: 0.7943\n",
      "Epoch 35: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4121 - accuracy: 0.8046 - val_loss: 0.5157 - val_accuracy: 0.7727\n",
      "Epoch 36/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.4277 - accuracy: 0.7964\n",
      "Epoch 36: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4106 - accuracy: 0.8094 - val_loss: 0.5169 - val_accuracy: 0.7727\n",
      "Epoch 37/350\n",
      "56/62 [==========================>...] - ETA: 0s - loss: 0.4217 - accuracy: 0.8000\n",
      "Epoch 37: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4091 - accuracy: 0.8094 - val_loss: 0.5177 - val_accuracy: 0.7662\n",
      "Epoch 38/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.4278 - accuracy: 0.7940\n",
      "Epoch 38: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4077 - accuracy: 0.8127 - val_loss: 0.5188 - val_accuracy: 0.7727\n",
      "Epoch 39/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.4201 - accuracy: 0.8019\n",
      "Epoch 39: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4063 - accuracy: 0.8127 - val_loss: 0.5200 - val_accuracy: 0.7662\n",
      "Epoch 40/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.4187 - accuracy: 0.8038\n",
      "Epoch 40: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4051 - accuracy: 0.8143 - val_loss: 0.5205 - val_accuracy: 0.7662\n",
      "Epoch 41/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.4203 - accuracy: 0.8000\n",
      "Epoch 41: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4038 - accuracy: 0.8127 - val_loss: 0.5211 - val_accuracy: 0.7662\n",
      "Epoch 42/350\n",
      "56/62 [==========================>...] - ETA: 0s - loss: 0.4146 - accuracy: 0.8036\n",
      "Epoch 42: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4026 - accuracy: 0.8127 - val_loss: 0.5220 - val_accuracy: 0.7662\n",
      "Epoch 43/350\n",
      "44/62 [====================>.........] - ETA: 0s - loss: 0.4235 - accuracy: 0.7955\n",
      "Epoch 43: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4013 - accuracy: 0.8143 - val_loss: 0.5230 - val_accuracy: 0.7662\n",
      "Epoch 44/350\n",
      "58/62 [===========================>..] - ETA: 0s - loss: 0.4100 - accuracy: 0.8069\n",
      "Epoch 44: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4002 - accuracy: 0.8160 - val_loss: 0.5233 - val_accuracy: 0.7662\n",
      "Epoch 45/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.4150 - accuracy: 0.8018\n",
      "Epoch 45: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3990 - accuracy: 0.8160 - val_loss: 0.5239 - val_accuracy: 0.7662\n",
      "Epoch 46/350\n",
      "51/62 [=======================>......] - ETA: 0s - loss: 0.4164 - accuracy: 0.8039\n",
      "Epoch 46: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3977 - accuracy: 0.8192 - val_loss: 0.5255 - val_accuracy: 0.7662\n",
      "Epoch 47/350\n",
      "49/62 [======================>.......] - ETA: 0s - loss: 0.4163 - accuracy: 0.8041\n",
      "Epoch 47: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3966 - accuracy: 0.8208 - val_loss: 0.5255 - val_accuracy: 0.7662\n",
      "Epoch 48/350\n",
      "42/62 [===================>..........] - ETA: 0s - loss: 0.4252 - accuracy: 0.7976\n",
      "Epoch 48: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3954 - accuracy: 0.8192 - val_loss: 0.5263 - val_accuracy: 0.7662\n",
      "Epoch 49/350\n",
      "49/62 [======================>.......] - ETA: 0s - loss: 0.4135 - accuracy: 0.8041\n",
      "Epoch 49: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3941 - accuracy: 0.8208 - val_loss: 0.5276 - val_accuracy: 0.7662\n",
      "Epoch 50/350\n",
      "47/62 [=====================>........] - ETA: 0s - loss: 0.4115 - accuracy: 0.8064\n",
      "Epoch 50: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3931 - accuracy: 0.8208 - val_loss: 0.5281 - val_accuracy: 0.7662\n",
      "Epoch 51/350\n",
      "48/62 [======================>.......] - ETA: 0s - loss: 0.4102 - accuracy: 0.8042\n",
      "Epoch 51: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3919 - accuracy: 0.8192 - val_loss: 0.5288 - val_accuracy: 0.7662\n",
      "Epoch 52/350\n",
      "57/62 [==========================>...] - ETA: 0s - loss: 0.4025 - accuracy: 0.8088\n",
      "Epoch 52: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3909 - accuracy: 0.8208 - val_loss: 0.5294 - val_accuracy: 0.7662\n",
      "Epoch 53/350\n",
      "51/62 [=======================>......] - ETA: 0s - loss: 0.4075 - accuracy: 0.8098\n",
      "Epoch 53: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3899 - accuracy: 0.8241 - val_loss: 0.5299 - val_accuracy: 0.7662\n",
      "Epoch 54/350\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.3956 - accuracy: 0.8186\n",
      "Epoch 54: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3889 - accuracy: 0.8241 - val_loss: 0.5298 - val_accuracy: 0.7662\n",
      "Epoch 55/350\n",
      "56/62 [==========================>...] - ETA: 0s - loss: 0.3987 - accuracy: 0.8125\n",
      "Epoch 55: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3876 - accuracy: 0.8225 - val_loss: 0.5311 - val_accuracy: 0.7727\n",
      "Epoch 56/350\n",
      "35/62 [===============>..............] - ETA: 0s - loss: 0.4215 - accuracy: 0.7971\n",
      "Epoch 56: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3866 - accuracy: 0.8257 - val_loss: 0.5311 - val_accuracy: 0.7727\n",
      "Epoch 57/350\n",
      "56/62 [==========================>...] - ETA: 0s - loss: 0.3965 - accuracy: 0.8161\n",
      "Epoch 57: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3854 - accuracy: 0.8257 - val_loss: 0.5319 - val_accuracy: 0.7727\n",
      "Epoch 58/350\n",
      "38/62 [=================>............] - ETA: 0s - loss: 0.4115 - accuracy: 0.8053\n",
      "Epoch 58: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3843 - accuracy: 0.8274 - val_loss: 0.5323 - val_accuracy: 0.7727\n",
      "Epoch 59/350\n",
      "41/62 [==================>...........] - ETA: 0s - loss: 0.4097 - accuracy: 0.8073\n",
      "Epoch 59: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3832 - accuracy: 0.8306 - val_loss: 0.5323 - val_accuracy: 0.7727\n",
      "Epoch 60/350\n",
      "44/62 [====================>.........] - ETA: 0s - loss: 0.4025 - accuracy: 0.8205\n",
      "Epoch 60: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3820 - accuracy: 0.8355 - val_loss: 0.5333 - val_accuracy: 0.7727\n",
      "Epoch 61/350\n",
      "48/62 [======================>.......] - ETA: 0s - loss: 0.3988 - accuracy: 0.8250\n",
      "Epoch 61: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3809 - accuracy: 0.8371 - val_loss: 0.5342 - val_accuracy: 0.7727\n",
      "Epoch 62/350\n",
      "51/62 [=======================>......] - ETA: 0s - loss: 0.3966 - accuracy: 0.8216\n",
      "Epoch 62: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3799 - accuracy: 0.8355 - val_loss: 0.5354 - val_accuracy: 0.7727\n",
      "Epoch 63/350\n",
      "54/62 [=========================>....] - ETA: 0s - loss: 0.3897 - accuracy: 0.8259\n",
      "Epoch 63: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3789 - accuracy: 0.8355 - val_loss: 0.5355 - val_accuracy: 0.7727\n",
      "Epoch 64/350\n",
      "42/62 [===================>..........] - ETA: 0s - loss: 0.4069 - accuracy: 0.8119\n",
      "Epoch 64: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3779 - accuracy: 0.8339 - val_loss: 0.5351 - val_accuracy: 0.7727\n",
      "Epoch 65/350\n",
      "56/62 [==========================>...] - ETA: 0s - loss: 0.3876 - accuracy: 0.8250\n",
      "Epoch 65: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3766 - accuracy: 0.8339 - val_loss: 0.5363 - val_accuracy: 0.7792\n",
      "Epoch 66/350\n",
      "44/62 [====================>.........] - ETA: 0s - loss: 0.3959 - accuracy: 0.8205\n",
      "Epoch 66: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3757 - accuracy: 0.8339 - val_loss: 0.5365 - val_accuracy: 0.7792\n",
      "Epoch 67/350\n",
      "45/62 [====================>.........] - ETA: 0s - loss: 0.3951 - accuracy: 0.8200\n",
      "Epoch 67: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3745 - accuracy: 0.8339 - val_loss: 0.5369 - val_accuracy: 0.7792\n",
      "Epoch 68/350\n",
      "30/62 [=============>................] - ETA: 0s - loss: 0.4010 - accuracy: 0.8133\n",
      "Epoch 68: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3734 - accuracy: 0.8355 - val_loss: 0.5367 - val_accuracy: 0.7792\n",
      "Epoch 69/350\n",
      "52/62 [========================>.....] - ETA: 0s - loss: 0.3885 - accuracy: 0.8250\n",
      "Epoch 69: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3724 - accuracy: 0.8371 - val_loss: 0.5377 - val_accuracy: 0.7792\n",
      "Epoch 70/350\n",
      "44/62 [====================>.........] - ETA: 0s - loss: 0.3914 - accuracy: 0.8250\n",
      "Epoch 70: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3713 - accuracy: 0.8388 - val_loss: 0.5379 - val_accuracy: 0.7792\n",
      "Epoch 71/350\n",
      "49/62 [======================>.......] - ETA: 0s - loss: 0.3879 - accuracy: 0.8265\n",
      "Epoch 71: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3701 - accuracy: 0.8371 - val_loss: 0.5392 - val_accuracy: 0.7792\n",
      "Epoch 72/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.3862 - accuracy: 0.8280\n",
      "Epoch 72: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3692 - accuracy: 0.8404 - val_loss: 0.5401 - val_accuracy: 0.7792\n",
      "Epoch 73/350\n",
      "48/62 [======================>.......] - ETA: 0s - loss: 0.3861 - accuracy: 0.8313\n",
      "Epoch 73: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3682 - accuracy: 0.8404 - val_loss: 0.5404 - val_accuracy: 0.7857\n",
      "Epoch 74/350\n",
      "48/62 [======================>.......] - ETA: 0s - loss: 0.3850 - accuracy: 0.8313\n",
      "Epoch 74: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3673 - accuracy: 0.8404 - val_loss: 0.5414 - val_accuracy: 0.7857\n",
      "Epoch 75/350\n",
      "47/62 [=====================>........] - ETA: 0s - loss: 0.3841 - accuracy: 0.8319\n",
      "Epoch 75: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3662 - accuracy: 0.8404 - val_loss: 0.5418 - val_accuracy: 0.7857\n",
      "Epoch 76/350\n",
      "48/62 [======================>.......] - ETA: 0s - loss: 0.3829 - accuracy: 0.8292\n",
      "Epoch 76: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3653 - accuracy: 0.8388 - val_loss: 0.5420 - val_accuracy: 0.7857\n",
      "Epoch 77/350\n",
      "41/62 [==================>...........] - ETA: 0s - loss: 0.3898 - accuracy: 0.8220\n",
      "Epoch 77: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3642 - accuracy: 0.8404 - val_loss: 0.5429 - val_accuracy: 0.7857\n",
      "Epoch 78/350\n",
      "56/62 [==========================>...] - ETA: 0s - loss: 0.3739 - accuracy: 0.8321\n",
      "Epoch 78: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3632 - accuracy: 0.8404 - val_loss: 0.5432 - val_accuracy: 0.7857\n",
      "Epoch 79/350\n",
      "56/62 [==========================>...] - ETA: 0s - loss: 0.3731 - accuracy: 0.8286\n",
      "Epoch 79: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3624 - accuracy: 0.8371 - val_loss: 0.5443 - val_accuracy: 0.7857\n",
      "Epoch 80/350\n",
      "49/62 [======================>.......] - ETA: 0s - loss: 0.3783 - accuracy: 0.8286\n",
      "Epoch 80: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3613 - accuracy: 0.8388 - val_loss: 0.5448 - val_accuracy: 0.7857\n",
      "Epoch 81/350\n",
      "52/62 [========================>.....] - ETA: 0s - loss: 0.3755 - accuracy: 0.8269\n",
      "Epoch 81: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3603 - accuracy: 0.8388 - val_loss: 0.5442 - val_accuracy: 0.7857\n",
      "Epoch 82/350\n",
      "46/62 [=====================>........] - ETA: 0s - loss: 0.3756 - accuracy: 0.8304\n",
      "Epoch 82: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3592 - accuracy: 0.8388 - val_loss: 0.5455 - val_accuracy: 0.7857\n",
      "Epoch 83/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.3746 - accuracy: 0.8280\n",
      "Epoch 83: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3583 - accuracy: 0.8388 - val_loss: 0.5459 - val_accuracy: 0.7792\n",
      "Epoch 84/350\n",
      "44/62 [====================>.........] - ETA: 0s - loss: 0.3768 - accuracy: 0.8318\n",
      "Epoch 84: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3573 - accuracy: 0.8404 - val_loss: 0.5465 - val_accuracy: 0.7792\n",
      "Epoch 85/350\n",
      "42/62 [===================>..........] - ETA: 0s - loss: 0.3843 - accuracy: 0.8238\n",
      "Epoch 85: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3563 - accuracy: 0.8388 - val_loss: 0.5477 - val_accuracy: 0.7792\n",
      "Epoch 86/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.3715 - accuracy: 0.8300\n",
      "Epoch 86: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3555 - accuracy: 0.8388 - val_loss: 0.5474 - val_accuracy: 0.7792\n",
      "Epoch 87/350\n",
      "51/62 [=======================>......] - ETA: 0s - loss: 0.3692 - accuracy: 0.8294\n",
      "Epoch 87: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3544 - accuracy: 0.8371 - val_loss: 0.5483 - val_accuracy: 0.7857\n",
      "Epoch 88/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.3633 - accuracy: 0.8358\n",
      "Epoch 88: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3533 - accuracy: 0.8436 - val_loss: 0.5487 - val_accuracy: 0.7857\n",
      "Epoch 89/350\n",
      "57/62 [==========================>...] - ETA: 0s - loss: 0.3629 - accuracy: 0.8281\n",
      "Epoch 89: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3524 - accuracy: 0.8388 - val_loss: 0.5488 - val_accuracy: 0.7792\n",
      "Epoch 90/350\n",
      "57/62 [==========================>...] - ETA: 0s - loss: 0.3618 - accuracy: 0.8298\n",
      "Epoch 90: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3512 - accuracy: 0.8404 - val_loss: 0.5508 - val_accuracy: 0.7792\n",
      "Epoch 91/350\n",
      "49/62 [======================>.......] - ETA: 0s - loss: 0.3668 - accuracy: 0.8367\n",
      "Epoch 91: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3504 - accuracy: 0.8436 - val_loss: 0.5510 - val_accuracy: 0.7792\n",
      "Epoch 92/350\n",
      "24/62 [==========>...................] - ETA: 0s - loss: 0.3746 - accuracy: 0.8292\n",
      "Epoch 92: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3491 - accuracy: 0.8420 - val_loss: 0.5514 - val_accuracy: 0.7792\n",
      "Epoch 93/350\n",
      "58/62 [===========================>..] - ETA: 0s - loss: 0.3564 - accuracy: 0.8379\n",
      "Epoch 93: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3481 - accuracy: 0.8453 - val_loss: 0.5528 - val_accuracy: 0.7727\n",
      "Epoch 94/350\n",
      "60/62 [============================>.] - ETA: 0s - loss: 0.3509 - accuracy: 0.8417\n",
      "Epoch 94: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3470 - accuracy: 0.8453 - val_loss: 0.5531 - val_accuracy: 0.7727\n",
      "Epoch 95/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.3606 - accuracy: 0.8364\n",
      "Epoch 95: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3461 - accuracy: 0.8469 - val_loss: 0.5537 - val_accuracy: 0.7662\n",
      "Epoch 96/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.3604 - accuracy: 0.8400\n",
      "Epoch 96: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3448 - accuracy: 0.8453 - val_loss: 0.5549 - val_accuracy: 0.7662\n",
      "Epoch 97/350\n",
      "58/62 [===========================>..] - ETA: 0s - loss: 0.3520 - accuracy: 0.8397\n",
      "Epoch 97: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3438 - accuracy: 0.8469 - val_loss: 0.5561 - val_accuracy: 0.7662\n",
      "Epoch 98/350\n",
      "52/62 [========================>.....] - ETA: 0s - loss: 0.3570 - accuracy: 0.8404\n",
      "Epoch 98: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3429 - accuracy: 0.8485 - val_loss: 0.5558 - val_accuracy: 0.7662\n",
      "Epoch 99/350\n",
      "52/62 [========================>.....] - ETA: 0s - loss: 0.3560 - accuracy: 0.8423\n",
      "Epoch 99: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3419 - accuracy: 0.8502 - val_loss: 0.5566 - val_accuracy: 0.7662\n",
      "Epoch 100/350\n",
      "30/62 [=============>................] - ETA: 0s - loss: 0.3643 - accuracy: 0.8467\n",
      "Epoch 100: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3408 - accuracy: 0.8518 - val_loss: 0.5576 - val_accuracy: 0.7662\n",
      "Epoch 101/350\n",
      "52/62 [========================>.....] - ETA: 0s - loss: 0.3539 - accuracy: 0.8442\n",
      "Epoch 101: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3400 - accuracy: 0.8502 - val_loss: 0.5589 - val_accuracy: 0.7662\n",
      "Epoch 102/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.3531 - accuracy: 0.8418\n",
      "Epoch 102: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3386 - accuracy: 0.8518 - val_loss: 0.5589 - val_accuracy: 0.7662\n",
      "Epoch 103/350\n",
      "54/62 [=========================>....] - ETA: 0s - loss: 0.3487 - accuracy: 0.8444\n",
      "Epoch 103: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3380 - accuracy: 0.8518 - val_loss: 0.5598 - val_accuracy: 0.7662\n",
      "Epoch 104/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.3521 - accuracy: 0.8460\n",
      "Epoch 104: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3368 - accuracy: 0.8502 - val_loss: 0.5598 - val_accuracy: 0.7662\n",
      "Epoch 105/350\n",
      "56/62 [==========================>...] - ETA: 0s - loss: 0.3461 - accuracy: 0.8464\n",
      "Epoch 105: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3360 - accuracy: 0.8502 - val_loss: 0.5602 - val_accuracy: 0.7662\n",
      "Epoch 106/350\n",
      "51/62 [=======================>......] - ETA: 0s - loss: 0.3486 - accuracy: 0.8490\n",
      "Epoch 106: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3347 - accuracy: 0.8518 - val_loss: 0.5602 - val_accuracy: 0.7662\n",
      "Epoch 107/350\n",
      "49/62 [======================>.......] - ETA: 0s - loss: 0.3495 - accuracy: 0.8449\n",
      "Epoch 107: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3340 - accuracy: 0.8485 - val_loss: 0.5611 - val_accuracy: 0.7662\n",
      "Epoch 108/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.3422 - accuracy: 0.8453\n",
      "Epoch 108: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3328 - accuracy: 0.8485 - val_loss: 0.5619 - val_accuracy: 0.7727\n",
      "Epoch 109/350\n",
      "37/62 [================>.............] - ETA: 0s - loss: 0.3596 - accuracy: 0.8432\n",
      "Epoch 109: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3322 - accuracy: 0.8469 - val_loss: 0.5623 - val_accuracy: 0.7727\n",
      "Epoch 110/350\n",
      "56/62 [==========================>...] - ETA: 0s - loss: 0.3409 - accuracy: 0.8429\n",
      "Epoch 110: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3309 - accuracy: 0.8469 - val_loss: 0.5639 - val_accuracy: 0.7727\n",
      "Epoch 111/350\n",
      "60/62 [============================>.] - ETA: 0s - loss: 0.3336 - accuracy: 0.8467\n",
      "Epoch 111: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.3300 - accuracy: 0.8485 - val_loss: 0.5643 - val_accuracy: 0.7727\n",
      "Epoch 112/350\n",
      "41/62 [==================>...........] - ETA: 0s - loss: 0.3529 - accuracy: 0.8415\n",
      "Epoch 112: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3290 - accuracy: 0.8502 - val_loss: 0.5641 - val_accuracy: 0.7727\n",
      "Epoch 113/350\n",
      "46/62 [=====================>........] - ETA: 0s - loss: 0.3425 - accuracy: 0.8478\n",
      "Epoch 113: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3278 - accuracy: 0.8502 - val_loss: 0.5655 - val_accuracy: 0.7727\n",
      "Epoch 114/350\n",
      "54/62 [=========================>....] - ETA: 0s - loss: 0.3374 - accuracy: 0.8444\n",
      "Epoch 114: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3270 - accuracy: 0.8485 - val_loss: 0.5670 - val_accuracy: 0.7727\n",
      "Epoch 115/350\n",
      "52/62 [========================>.....] - ETA: 0s - loss: 0.3386 - accuracy: 0.8481\n",
      "Epoch 115: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3257 - accuracy: 0.8518 - val_loss: 0.5662 - val_accuracy: 0.7792\n",
      "Epoch 116/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.3336 - accuracy: 0.8472\n",
      "Epoch 116: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3247 - accuracy: 0.8502 - val_loss: 0.5678 - val_accuracy: 0.7727\n",
      "Epoch 117/350\n",
      "47/62 [=====================>........] - ETA: 0s - loss: 0.3391 - accuracy: 0.8489\n",
      "Epoch 117: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3235 - accuracy: 0.8518 - val_loss: 0.5680 - val_accuracy: 0.7792\n",
      "Epoch 118/350\n",
      "45/62 [====================>.........] - ETA: 0s - loss: 0.3409 - accuracy: 0.8489\n",
      "Epoch 118: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3223 - accuracy: 0.8518 - val_loss: 0.5695 - val_accuracy: 0.7792\n",
      "Epoch 119/350\n",
      "51/62 [=======================>......] - ETA: 0s - loss: 0.3338 - accuracy: 0.8490\n",
      "Epoch 119: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3212 - accuracy: 0.8518 - val_loss: 0.5711 - val_accuracy: 0.7792\n",
      "Epoch 120/350\n",
      "51/62 [=======================>......] - ETA: 0s - loss: 0.3328 - accuracy: 0.8490\n",
      "Epoch 120: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3203 - accuracy: 0.8518 - val_loss: 0.5708 - val_accuracy: 0.7792\n",
      "Epoch 121/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.3277 - accuracy: 0.8491\n",
      "Epoch 121: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3190 - accuracy: 0.8518 - val_loss: 0.5723 - val_accuracy: 0.7792\n",
      "Epoch 122/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.3319 - accuracy: 0.8460\n",
      "Epoch 122: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3179 - accuracy: 0.8502 - val_loss: 0.5727 - val_accuracy: 0.7792\n",
      "Epoch 123/350\n",
      "43/62 [===================>..........] - ETA: 0s - loss: 0.3388 - accuracy: 0.8442\n",
      "Epoch 123: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3170 - accuracy: 0.8502 - val_loss: 0.5741 - val_accuracy: 0.7792\n",
      "Epoch 124/350\n",
      "44/62 [====================>.........] - ETA: 0s - loss: 0.3341 - accuracy: 0.8500\n",
      "Epoch 124: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3161 - accuracy: 0.8502 - val_loss: 0.5741 - val_accuracy: 0.7792\n",
      "Epoch 125/350\n",
      "47/62 [=====================>........] - ETA: 0s - loss: 0.3299 - accuracy: 0.8468\n",
      "Epoch 125: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3148 - accuracy: 0.8502 - val_loss: 0.5759 - val_accuracy: 0.7792\n",
      "Epoch 126/350\n",
      "47/62 [=====================>........] - ETA: 0s - loss: 0.3286 - accuracy: 0.8468\n",
      "Epoch 126: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3136 - accuracy: 0.8502 - val_loss: 0.5772 - val_accuracy: 0.7792\n",
      "Epoch 127/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.3260 - accuracy: 0.8460\n",
      "Epoch 127: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3125 - accuracy: 0.8502 - val_loss: 0.5789 - val_accuracy: 0.7727\n",
      "Epoch 128/350\n",
      "45/62 [====================>.........] - ETA: 0s - loss: 0.3292 - accuracy: 0.8511\n",
      "Epoch 128: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3116 - accuracy: 0.8518 - val_loss: 0.5796 - val_accuracy: 0.7792\n",
      "Epoch 129/350\n",
      "48/62 [======================>.......] - ETA: 0s - loss: 0.3260 - accuracy: 0.8458\n",
      "Epoch 129: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3104 - accuracy: 0.8502 - val_loss: 0.5811 - val_accuracy: 0.7792\n",
      "Epoch 130/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.3228 - accuracy: 0.8460\n",
      "Epoch 130: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3095 - accuracy: 0.8502 - val_loss: 0.5812 - val_accuracy: 0.7792\n",
      "Epoch 131/350\n",
      "48/62 [======================>.......] - ETA: 0s - loss: 0.3236 - accuracy: 0.8458\n",
      "Epoch 131: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3082 - accuracy: 0.8502 - val_loss: 0.5808 - val_accuracy: 0.7792\n",
      "Epoch 132/350\n",
      "54/62 [=========================>....] - ETA: 0s - loss: 0.3169 - accuracy: 0.8481\n",
      "Epoch 132: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3073 - accuracy: 0.8518 - val_loss: 0.5830 - val_accuracy: 0.7727\n",
      "Epoch 133/350\n",
      "51/62 [=======================>......] - ETA: 0s - loss: 0.3177 - accuracy: 0.8490\n",
      "Epoch 133: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3063 - accuracy: 0.8534 - val_loss: 0.5830 - val_accuracy: 0.7727\n",
      "Epoch 134/350\n",
      "44/62 [====================>.........] - ETA: 0s - loss: 0.3224 - accuracy: 0.8568\n",
      "Epoch 134: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3052 - accuracy: 0.8567 - val_loss: 0.5846 - val_accuracy: 0.7727\n",
      "Epoch 135/350\n",
      "56/62 [==========================>...] - ETA: 0s - loss: 0.3130 - accuracy: 0.8518\n",
      "Epoch 135: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3042 - accuracy: 0.8550 - val_loss: 0.5849 - val_accuracy: 0.7727\n",
      "Epoch 136/350\n",
      "49/62 [======================>.......] - ETA: 0s - loss: 0.3169 - accuracy: 0.8531\n",
      "Epoch 136: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3033 - accuracy: 0.8567 - val_loss: 0.5851 - val_accuracy: 0.7727\n",
      "Epoch 137/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.3148 - accuracy: 0.8520\n",
      "Epoch 137: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3019 - accuracy: 0.8550 - val_loss: 0.5871 - val_accuracy: 0.7727\n",
      "Epoch 138/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.3136 - accuracy: 0.8509\n",
      "Epoch 138: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3012 - accuracy: 0.8567 - val_loss: 0.5863 - val_accuracy: 0.7727\n",
      "Epoch 139/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.3124 - accuracy: 0.8491\n",
      "Epoch 139: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3000 - accuracy: 0.8550 - val_loss: 0.5875 - val_accuracy: 0.7727\n",
      "Epoch 140/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.3113 - accuracy: 0.8560\n",
      "Epoch 140: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2989 - accuracy: 0.8583 - val_loss: 0.5899 - val_accuracy: 0.7727\n",
      "Epoch 141/350\n",
      "43/62 [===================>..........] - ETA: 0s - loss: 0.3182 - accuracy: 0.8558\n",
      "Epoch 141: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2983 - accuracy: 0.8583 - val_loss: 0.5892 - val_accuracy: 0.7727\n",
      "Epoch 142/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.3043 - accuracy: 0.8528\n",
      "Epoch 142: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2969 - accuracy: 0.8567 - val_loss: 0.5901 - val_accuracy: 0.7792\n",
      "Epoch 143/350\n",
      "47/62 [=====================>........] - ETA: 0s - loss: 0.3097 - accuracy: 0.8553\n",
      "Epoch 143: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2963 - accuracy: 0.8583 - val_loss: 0.5920 - val_accuracy: 0.7792\n",
      "Epoch 144/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.3023 - accuracy: 0.8585\n",
      "Epoch 144: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2951 - accuracy: 0.8616 - val_loss: 0.5929 - val_accuracy: 0.7792\n",
      "Epoch 145/350\n",
      "46/62 [=====================>........] - ETA: 0s - loss: 0.3055 - accuracy: 0.8587\n",
      "Epoch 145: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2943 - accuracy: 0.8583 - val_loss: 0.5937 - val_accuracy: 0.7792\n",
      "Epoch 146/350\n",
      "52/62 [========================>.....] - ETA: 0s - loss: 0.3028 - accuracy: 0.8635\n",
      "Epoch 146: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2931 - accuracy: 0.8664 - val_loss: 0.5949 - val_accuracy: 0.7792\n",
      "Epoch 147/350\n",
      "52/62 [========================>.....] - ETA: 0s - loss: 0.3020 - accuracy: 0.8654\n",
      "Epoch 147: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2924 - accuracy: 0.8681 - val_loss: 0.5960 - val_accuracy: 0.7857\n",
      "Epoch 148/350\n",
      "49/62 [======================>.......] - ETA: 0s - loss: 0.3034 - accuracy: 0.8714\n",
      "Epoch 148: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2912 - accuracy: 0.8713 - val_loss: 0.5985 - val_accuracy: 0.7792\n",
      "Epoch 149/350\n",
      "41/62 [==================>...........] - ETA: 0s - loss: 0.3114 - accuracy: 0.8659\n",
      "Epoch 149: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2905 - accuracy: 0.8681 - val_loss: 0.5998 - val_accuracy: 0.7792\n",
      "Epoch 150/350\n",
      "46/62 [=====================>........] - ETA: 0s - loss: 0.2999 - accuracy: 0.8717\n",
      "Epoch 150: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2894 - accuracy: 0.8697 - val_loss: 0.6018 - val_accuracy: 0.7857\n",
      "Epoch 151/350\n",
      "51/62 [=======================>......] - ETA: 0s - loss: 0.2979 - accuracy: 0.8686\n",
      "Epoch 151: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2887 - accuracy: 0.8697 - val_loss: 0.6024 - val_accuracy: 0.7792\n",
      "Epoch 152/350\n",
      "46/62 [=====================>........] - ETA: 0s - loss: 0.2981 - accuracy: 0.8717\n",
      "Epoch 152: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2877 - accuracy: 0.8697 - val_loss: 0.6044 - val_accuracy: 0.7857\n",
      "Epoch 153/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.2984 - accuracy: 0.8636\n",
      "Epoch 153: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2867 - accuracy: 0.8697 - val_loss: 0.6058 - val_accuracy: 0.7857\n",
      "Epoch 154/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.2925 - accuracy: 0.8679\n",
      "Epoch 154: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2860 - accuracy: 0.8697 - val_loss: 0.6078 - val_accuracy: 0.7857\n",
      "Epoch 155/350\n",
      "32/62 [==============>...............] - ETA: 0s - loss: 0.2932 - accuracy: 0.8844\n",
      "Epoch 155: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2848 - accuracy: 0.8730 - val_loss: 0.6078 - val_accuracy: 0.7857\n",
      "Epoch 156/350\n",
      "51/62 [=======================>......] - ETA: 0s - loss: 0.2931 - accuracy: 0.8725\n",
      "Epoch 156: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2843 - accuracy: 0.8730 - val_loss: 0.6091 - val_accuracy: 0.7857\n",
      "Epoch 157/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.2947 - accuracy: 0.8709\n",
      "Epoch 157: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2832 - accuracy: 0.8762 - val_loss: 0.6099 - val_accuracy: 0.7857\n",
      "Epoch 158/350\n",
      "51/62 [=======================>......] - ETA: 0s - loss: 0.2910 - accuracy: 0.8765\n",
      "Epoch 158: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2823 - accuracy: 0.8762 - val_loss: 0.6126 - val_accuracy: 0.7857\n",
      "Epoch 159/350\n",
      "43/62 [===================>..........] - ETA: 0s - loss: 0.2990 - accuracy: 0.8744\n",
      "Epoch 159: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2811 - accuracy: 0.8746 - val_loss: 0.6146 - val_accuracy: 0.7857\n",
      "Epoch 160/350\n",
      "44/62 [====================>.........] - ETA: 0s - loss: 0.2946 - accuracy: 0.8795\n",
      "Epoch 160: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2802 - accuracy: 0.8762 - val_loss: 0.6150 - val_accuracy: 0.7857\n",
      "Epoch 161/350\n",
      "52/62 [========================>.....] - ETA: 0s - loss: 0.2878 - accuracy: 0.8750\n",
      "Epoch 161: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2794 - accuracy: 0.8762 - val_loss: 0.6167 - val_accuracy: 0.7792\n",
      "Epoch 162/350\n",
      "51/62 [=======================>......] - ETA: 0s - loss: 0.2865 - accuracy: 0.8804\n",
      "Epoch 162: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2782 - accuracy: 0.8795 - val_loss: 0.6195 - val_accuracy: 0.7792\n",
      "Epoch 163/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.2838 - accuracy: 0.8811\n",
      "Epoch 163: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2776 - accuracy: 0.8811 - val_loss: 0.6206 - val_accuracy: 0.7792\n",
      "Epoch 164/350\n",
      "49/62 [======================>.......] - ETA: 0s - loss: 0.2878 - accuracy: 0.8816\n",
      "Epoch 164: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2766 - accuracy: 0.8844 - val_loss: 0.6227 - val_accuracy: 0.7792\n",
      "Epoch 165/350\n",
      "49/62 [======================>.......] - ETA: 0s - loss: 0.2867 - accuracy: 0.8837\n",
      "Epoch 165: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2755 - accuracy: 0.8844 - val_loss: 0.6235 - val_accuracy: 0.7857\n",
      "Epoch 166/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.2848 - accuracy: 0.8840\n",
      "Epoch 166: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2747 - accuracy: 0.8860 - val_loss: 0.6255 - val_accuracy: 0.7857\n",
      "Epoch 167/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.2837 - accuracy: 0.8860\n",
      "Epoch 167: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2736 - accuracy: 0.8876 - val_loss: 0.6264 - val_accuracy: 0.7857\n",
      "Epoch 168/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.2828 - accuracy: 0.8840\n",
      "Epoch 168: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2727 - accuracy: 0.8860 - val_loss: 0.6274 - val_accuracy: 0.7857\n",
      "Epoch 169/350\n",
      "51/62 [=======================>......] - ETA: 0s - loss: 0.2799 - accuracy: 0.8922\n",
      "Epoch 169: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2719 - accuracy: 0.8925 - val_loss: 0.6298 - val_accuracy: 0.7857\n",
      "Epoch 170/350\n",
      "54/62 [=========================>....] - ETA: 0s - loss: 0.2794 - accuracy: 0.8889\n",
      "Epoch 170: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2710 - accuracy: 0.8925 - val_loss: 0.6296 - val_accuracy: 0.7857\n",
      "Epoch 171/350\n",
      "51/62 [=======================>......] - ETA: 0s - loss: 0.2777 - accuracy: 0.8941\n",
      "Epoch 171: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2701 - accuracy: 0.8941 - val_loss: 0.6336 - val_accuracy: 0.7857\n",
      "Epoch 172/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.2799 - accuracy: 0.8873\n",
      "Epoch 172: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2691 - accuracy: 0.8925 - val_loss: 0.6324 - val_accuracy: 0.7857\n",
      "Epoch 173/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.2788 - accuracy: 0.8891\n",
      "Epoch 173: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2680 - accuracy: 0.8941 - val_loss: 0.6355 - val_accuracy: 0.7857\n",
      "Epoch 174/350\n",
      "34/62 [===============>..............] - ETA: 0s - loss: 0.2843 - accuracy: 0.8912\n",
      "Epoch 174: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2674 - accuracy: 0.8909 - val_loss: 0.6347 - val_accuracy: 0.7857\n",
      "Epoch 175/350\n",
      "51/62 [=======================>......] - ETA: 0s - loss: 0.2736 - accuracy: 0.8941\n",
      "Epoch 175: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2662 - accuracy: 0.8941 - val_loss: 0.6377 - val_accuracy: 0.7857\n",
      "Epoch 176/350\n",
      "47/62 [=====================>........] - ETA: 0s - loss: 0.2765 - accuracy: 0.8915\n",
      "Epoch 176: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2655 - accuracy: 0.8941 - val_loss: 0.6376 - val_accuracy: 0.7857\n",
      "Epoch 177/350\n",
      "36/62 [================>.............] - ETA: 0s - loss: 0.2783 - accuracy: 0.9000\n",
      "Epoch 177: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2646 - accuracy: 0.8958 - val_loss: 0.6405 - val_accuracy: 0.7857\n",
      "Epoch 178/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.2731 - accuracy: 0.8940\n",
      "Epoch 178: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2636 - accuracy: 0.8958 - val_loss: 0.6412 - val_accuracy: 0.7857\n",
      "Epoch 179/350\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.2639 - accuracy: 0.8934\n",
      "Epoch 179: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2627 - accuracy: 0.8941 - val_loss: 0.6418 - val_accuracy: 0.7857\n",
      "Epoch 180/350\n",
      "41/62 [==================>...........] - ETA: 0s - loss: 0.2814 - accuracy: 0.8976\n",
      "Epoch 180: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2616 - accuracy: 0.8974 - val_loss: 0.6453 - val_accuracy: 0.7857\n",
      "Epoch 181/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.2663 - accuracy: 0.8962\n",
      "Epoch 181: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8974 - val_loss: 0.6439 - val_accuracy: 0.7857\n",
      "Epoch 182/350\n",
      "44/62 [====================>.........] - ETA: 0s - loss: 0.2738 - accuracy: 0.9000\n",
      "Epoch 182: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2600 - accuracy: 0.8974 - val_loss: 0.6469 - val_accuracy: 0.7857\n",
      "Epoch 183/350\n",
      "57/62 [==========================>...] - ETA: 0s - loss: 0.2655 - accuracy: 0.8965\n",
      "Epoch 183: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2586 - accuracy: 0.8990 - val_loss: 0.6486 - val_accuracy: 0.7857\n",
      "Epoch 184/350\n",
      "47/62 [=====================>........] - ETA: 0s - loss: 0.2688 - accuracy: 0.8979\n",
      "Epoch 184: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2579 - accuracy: 0.8990 - val_loss: 0.6491 - val_accuracy: 0.7857\n",
      "Epoch 185/350\n",
      "36/62 [================>.............] - ETA: 0s - loss: 0.2699 - accuracy: 0.9056\n",
      "Epoch 185: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2573 - accuracy: 0.9007 - val_loss: 0.6513 - val_accuracy: 0.7857\n",
      "Epoch 186/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.2615 - accuracy: 0.9000\n",
      "Epoch 186: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2560 - accuracy: 0.9023 - val_loss: 0.6526 - val_accuracy: 0.7857\n",
      "Epoch 187/350\n",
      "52/62 [========================>.....] - ETA: 0s - loss: 0.2627 - accuracy: 0.9000\n",
      "Epoch 187: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2555 - accuracy: 0.9023 - val_loss: 0.6531 - val_accuracy: 0.7857\n",
      "Epoch 188/350\n",
      "46/62 [=====================>........] - ETA: 0s - loss: 0.2635 - accuracy: 0.9022\n",
      "Epoch 188: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2542 - accuracy: 0.9023 - val_loss: 0.6554 - val_accuracy: 0.7857\n",
      "Epoch 189/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.2640 - accuracy: 0.8982\n",
      "Epoch 189: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2535 - accuracy: 0.9023 - val_loss: 0.6564 - val_accuracy: 0.7857\n",
      "Epoch 190/350\n",
      "49/62 [======================>.......] - ETA: 0s - loss: 0.2630 - accuracy: 0.9000\n",
      "Epoch 190: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2527 - accuracy: 0.9023 - val_loss: 0.6594 - val_accuracy: 0.7857\n",
      "Epoch 191/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.2624 - accuracy: 0.8982\n",
      "Epoch 191: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2520 - accuracy: 0.9023 - val_loss: 0.6605 - val_accuracy: 0.7857\n",
      "Epoch 192/350\n",
      "62/62 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.9023\n",
      "Epoch 192: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2510 - accuracy: 0.9023 - val_loss: 0.6619 - val_accuracy: 0.7857\n",
      "Epoch 193/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.2553 - accuracy: 0.9000\n",
      "Epoch 193: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.9023 - val_loss: 0.6637 - val_accuracy: 0.7857\n",
      "Epoch 194/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.2596 - accuracy: 0.8982\n",
      "Epoch 194: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2493 - accuracy: 0.9023 - val_loss: 0.6655 - val_accuracy: 0.7922\n",
      "Epoch 195/350\n",
      "52/62 [========================>.....] - ETA: 0s - loss: 0.2554 - accuracy: 0.9000\n",
      "Epoch 195: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2486 - accuracy: 0.9023 - val_loss: 0.6681 - val_accuracy: 0.7857\n",
      "Epoch 196/350\n",
      "46/62 [=====================>........] - ETA: 0s - loss: 0.2563 - accuracy: 0.9022\n",
      "Epoch 196: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2474 - accuracy: 0.9023 - val_loss: 0.6672 - val_accuracy: 0.7922\n",
      "Epoch 197/350\n",
      "42/62 [===================>..........] - ETA: 0s - loss: 0.2671 - accuracy: 0.8976\n",
      "Epoch 197: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2468 - accuracy: 0.9023 - val_loss: 0.6715 - val_accuracy: 0.7857\n",
      "Epoch 198/350\n",
      "29/62 [=============>................] - ETA: 0s - loss: 0.2581 - accuracy: 0.9069\n",
      "Epoch 198: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2461 - accuracy: 0.9039 - val_loss: 0.6712 - val_accuracy: 0.7922\n",
      "Epoch 199/350\n",
      "39/62 [=================>............] - ETA: 0s - loss: 0.2558 - accuracy: 0.9051\n",
      "Epoch 199: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2449 - accuracy: 0.9039 - val_loss: 0.6729 - val_accuracy: 0.7922\n",
      "Epoch 200/350\n",
      "40/62 [==================>...........] - ETA: 0s - loss: 0.2587 - accuracy: 0.9025\n",
      "Epoch 200: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2445 - accuracy: 0.9023 - val_loss: 0.6772 - val_accuracy: 0.7922\n",
      "Epoch 201/350\n",
      "31/62 [==============>...............] - ETA: 0s - loss: 0.2514 - accuracy: 0.9097\n",
      "Epoch 201: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2435 - accuracy: 0.9039 - val_loss: 0.6761 - val_accuracy: 0.7922\n",
      "Epoch 202/350\n",
      "58/62 [===========================>..] - ETA: 0s - loss: 0.2481 - accuracy: 0.9017\n",
      "Epoch 202: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2426 - accuracy: 0.9055 - val_loss: 0.6788 - val_accuracy: 0.7922\n",
      "Epoch 203/350\n",
      "37/62 [================>.............] - ETA: 0s - loss: 0.2579 - accuracy: 0.9027\n",
      "Epoch 203: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2419 - accuracy: 0.9039 - val_loss: 0.6801 - val_accuracy: 0.7922\n",
      "Epoch 204/350\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.2449 - accuracy: 0.9051\n",
      "Epoch 204: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2410 - accuracy: 0.9072 - val_loss: 0.6818 - val_accuracy: 0.7922\n",
      "Epoch 205/350\n",
      "34/62 [===============>..............] - ETA: 0s - loss: 0.2530 - accuracy: 0.9059\n",
      "Epoch 205: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2405 - accuracy: 0.9072 - val_loss: 0.6838 - val_accuracy: 0.7922\n",
      "Epoch 206/350\n",
      "34/62 [===============>..............] - ETA: 0s - loss: 0.2520 - accuracy: 0.9088\n",
      "Epoch 206: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2395 - accuracy: 0.9088 - val_loss: 0.6845 - val_accuracy: 0.7857\n",
      "Epoch 207/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.2471 - accuracy: 0.9060\n",
      "Epoch 207: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2388 - accuracy: 0.9104 - val_loss: 0.6859 - val_accuracy: 0.7922\n",
      "Epoch 208/350\n",
      "34/62 [===============>..............] - ETA: 0s - loss: 0.2501 - accuracy: 0.9059\n",
      "Epoch 208: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2381 - accuracy: 0.9088 - val_loss: 0.6874 - val_accuracy: 0.7857\n",
      "Epoch 209/350\n",
      "32/62 [==============>...............] - ETA: 0s - loss: 0.2417 - accuracy: 0.9125\n",
      "Epoch 209: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2375 - accuracy: 0.9104 - val_loss: 0.6881 - val_accuracy: 0.7922\n",
      "Epoch 210/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.2443 - accuracy: 0.9060\n",
      "Epoch 210: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2362 - accuracy: 0.9104 - val_loss: 0.6932 - val_accuracy: 0.7922\n",
      "Epoch 211/350\n",
      "57/62 [==========================>...] - ETA: 0s - loss: 0.2425 - accuracy: 0.9070\n",
      "Epoch 211: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2360 - accuracy: 0.9121 - val_loss: 0.6919 - val_accuracy: 0.7857\n",
      "Epoch 212/350\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.2389 - accuracy: 0.9085\n",
      "Epoch 212: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2352 - accuracy: 0.9104 - val_loss: 0.6927 - val_accuracy: 0.7922\n",
      "Epoch 213/350\n",
      "60/62 [============================>.] - ETA: 0s - loss: 0.2356 - accuracy: 0.9117\n",
      "Epoch 213: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2339 - accuracy: 0.9121 - val_loss: 0.6959 - val_accuracy: 0.7922\n",
      "Epoch 214/350\n",
      "60/62 [============================>.] - ETA: 0s - loss: 0.2353 - accuracy: 0.9117\n",
      "Epoch 214: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.2335 - accuracy: 0.9121 - val_loss: 0.6995 - val_accuracy: 0.7922\n",
      "Epoch 215/350\n",
      "43/62 [===================>..........] - ETA: 0s - loss: 0.2482 - accuracy: 0.9070\n",
      "Epoch 215: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2329 - accuracy: 0.9121 - val_loss: 0.6993 - val_accuracy: 0.7922\n",
      "Epoch 216/350\n",
      "57/62 [==========================>...] - ETA: 0s - loss: 0.2380 - accuracy: 0.9088\n",
      "Epoch 216: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2317 - accuracy: 0.9137 - val_loss: 0.7005 - val_accuracy: 0.7987\n",
      "Epoch 217/350\n",
      "54/62 [=========================>....] - ETA: 0s - loss: 0.2392 - accuracy: 0.9037\n",
      "Epoch 217: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.2312 - accuracy: 0.9104 - val_loss: 0.7030 - val_accuracy: 0.7922\n",
      "Epoch 218/350\n",
      "35/62 [===============>..............] - ETA: 0s - loss: 0.2387 - accuracy: 0.9086\n",
      "Epoch 218: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2303 - accuracy: 0.9121 - val_loss: 0.7038 - val_accuracy: 0.7857\n",
      "Epoch 219/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.2393 - accuracy: 0.9036\n",
      "Epoch 219: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2295 - accuracy: 0.9104 - val_loss: 0.7057 - val_accuracy: 0.7857\n",
      "Epoch 220/350\n",
      "46/62 [=====================>........] - ETA: 0s - loss: 0.2365 - accuracy: 0.9109\n",
      "Epoch 220: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2289 - accuracy: 0.9121 - val_loss: 0.7072 - val_accuracy: 0.7857\n",
      "Epoch 221/350\n",
      "42/62 [===================>..........] - ETA: 0s - loss: 0.2465 - accuracy: 0.9024\n",
      "Epoch 221: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2283 - accuracy: 0.9104 - val_loss: 0.7077 - val_accuracy: 0.7857\n",
      "Epoch 222/350\n",
      "46/62 [=====================>........] - ETA: 0s - loss: 0.2346 - accuracy: 0.9130\n",
      "Epoch 222: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2271 - accuracy: 0.9137 - val_loss: 0.7111 - val_accuracy: 0.7922\n",
      "Epoch 223/350\n",
      "45/62 [====================>.........] - ETA: 0s - loss: 0.2377 - accuracy: 0.9089\n",
      "Epoch 223: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2265 - accuracy: 0.9121 - val_loss: 0.7111 - val_accuracy: 0.7857\n",
      "Epoch 224/350\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.2268 - accuracy: 0.9131\n",
      "Epoch 224: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2257 - accuracy: 0.9137 - val_loss: 0.7129 - val_accuracy: 0.7857\n",
      "Epoch 225/350\n",
      "57/62 [==========================>...] - ETA: 0s - loss: 0.2309 - accuracy: 0.9105\n",
      "Epoch 225: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2248 - accuracy: 0.9153 - val_loss: 0.7156 - val_accuracy: 0.7857\n",
      "Epoch 226/350\n",
      "54/62 [=========================>....] - ETA: 0s - loss: 0.2321 - accuracy: 0.9093\n",
      "Epoch 226: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2243 - accuracy: 0.9153 - val_loss: 0.7150 - val_accuracy: 0.7857\n",
      "Epoch 227/350\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.2244 - accuracy: 0.9148\n",
      "Epoch 227: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.2233 - accuracy: 0.9153 - val_loss: 0.7184 - val_accuracy: 0.7857\n",
      "Epoch 228/350\n",
      "45/62 [====================>.........] - ETA: 0s - loss: 0.2332 - accuracy: 0.9133\n",
      "Epoch 228: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2226 - accuracy: 0.9153 - val_loss: 0.7199 - val_accuracy: 0.7857\n",
      "Epoch 229/350\n",
      "44/62 [====================>.........] - ETA: 0s - loss: 0.2326 - accuracy: 0.9114\n",
      "Epoch 229: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2219 - accuracy: 0.9137 - val_loss: 0.7230 - val_accuracy: 0.7857\n",
      "Epoch 230/350\n",
      "46/62 [=====================>........] - ETA: 0s - loss: 0.2281 - accuracy: 0.9152\n",
      "Epoch 230: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2211 - accuracy: 0.9153 - val_loss: 0.7246 - val_accuracy: 0.7857\n",
      "Epoch 231/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.2247 - accuracy: 0.9132\n",
      "Epoch 231: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2204 - accuracy: 0.9169 - val_loss: 0.7258 - val_accuracy: 0.7857\n",
      "Epoch 232/350\n",
      "57/62 [==========================>...] - ETA: 0s - loss: 0.2255 - accuracy: 0.9123\n",
      "Epoch 232: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2195 - accuracy: 0.9169 - val_loss: 0.7272 - val_accuracy: 0.7857\n",
      "Epoch 233/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.2280 - accuracy: 0.9091\n",
      "Epoch 233: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2187 - accuracy: 0.9153 - val_loss: 0.7308 - val_accuracy: 0.7857\n",
      "Epoch 234/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.2275 - accuracy: 0.9127\n",
      "Epoch 234: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.2181 - accuracy: 0.9186 - val_loss: 0.7299 - val_accuracy: 0.7857\n",
      "Epoch 235/350\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.2185 - accuracy: 0.9180\n",
      "Epoch 235: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.2174 - accuracy: 0.9186 - val_loss: 0.7326 - val_accuracy: 0.7857\n",
      "Epoch 236/350\n",
      "56/62 [==========================>...] - ETA: 0s - loss: 0.2230 - accuracy: 0.9161\n",
      "Epoch 236: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.2166 - accuracy: 0.9202 - val_loss: 0.7360 - val_accuracy: 0.7857\n",
      "Epoch 237/350\n",
      "44/62 [====================>.........] - ETA: 0s - loss: 0.2262 - accuracy: 0.9182\n",
      "Epoch 237: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2160 - accuracy: 0.9186 - val_loss: 0.7371 - val_accuracy: 0.7857\n",
      "Epoch 238/350\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.2185 - accuracy: 0.9186\n",
      "Epoch 238: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2150 - accuracy: 0.9202 - val_loss: 0.7376 - val_accuracy: 0.7857\n",
      "Epoch 239/350\n",
      "44/62 [====================>.........] - ETA: 0s - loss: 0.2252 - accuracy: 0.9182\n",
      "Epoch 239: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2145 - accuracy: 0.9186 - val_loss: 0.7404 - val_accuracy: 0.7857\n",
      "Epoch 240/350\n",
      "41/62 [==================>...........] - ETA: 0s - loss: 0.2293 - accuracy: 0.9146\n",
      "Epoch 240: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2137 - accuracy: 0.9186 - val_loss: 0.7409 - val_accuracy: 0.7857\n",
      "Epoch 241/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.2192 - accuracy: 0.9180\n",
      "Epoch 241: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2127 - accuracy: 0.9202 - val_loss: 0.7441 - val_accuracy: 0.7857\n",
      "Epoch 242/350\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.2154 - accuracy: 0.9169\n",
      "Epoch 242: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2120 - accuracy: 0.9186 - val_loss: 0.7435 - val_accuracy: 0.7857\n",
      "Epoch 243/350\n",
      "47/62 [=====================>........] - ETA: 0s - loss: 0.2188 - accuracy: 0.9191\n",
      "Epoch 243: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2115 - accuracy: 0.9202 - val_loss: 0.7471 - val_accuracy: 0.7857\n",
      "Epoch 244/350\n",
      "62/62 [==============================] - ETA: 0s - loss: 0.2104 - accuracy: 0.9218\n",
      "Epoch 244: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2104 - accuracy: 0.9218 - val_loss: 0.7472 - val_accuracy: 0.7857\n",
      "Epoch 245/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.2139 - accuracy: 0.9189\n",
      "Epoch 245: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.2100 - accuracy: 0.9218 - val_loss: 0.7477 - val_accuracy: 0.7857\n",
      "Epoch 246/350\n",
      "47/62 [=====================>........] - ETA: 0s - loss: 0.2163 - accuracy: 0.9170\n",
      "Epoch 246: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2092 - accuracy: 0.9202 - val_loss: 0.7499 - val_accuracy: 0.7857\n",
      "Epoch 247/350\n",
      "57/62 [==========================>...] - ETA: 0s - loss: 0.2143 - accuracy: 0.9175\n",
      "Epoch 247: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.2086 - accuracy: 0.9218 - val_loss: 0.7527 - val_accuracy: 0.7857\n",
      "Epoch 248/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.2114 - accuracy: 0.9208\n",
      "Epoch 248: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.2078 - accuracy: 0.9235 - val_loss: 0.7515 - val_accuracy: 0.7857\n",
      "Epoch 249/350\n",
      "49/62 [======================>.......] - ETA: 0s - loss: 0.2142 - accuracy: 0.9204\n",
      "Epoch 249: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2069 - accuracy: 0.9235 - val_loss: 0.7551 - val_accuracy: 0.7857\n",
      "Epoch 250/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.2121 - accuracy: 0.9220\n",
      "Epoch 250: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.2061 - accuracy: 0.9235 - val_loss: 0.7546 - val_accuracy: 0.7857\n",
      "Epoch 251/350\n",
      "56/62 [==========================>...] - ETA: 0s - loss: 0.2115 - accuracy: 0.9214\n",
      "Epoch 251: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2056 - accuracy: 0.9251 - val_loss: 0.7578 - val_accuracy: 0.7857\n",
      "Epoch 252/350\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.2081 - accuracy: 0.9237\n",
      "Epoch 252: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2048 - accuracy: 0.9251 - val_loss: 0.7580 - val_accuracy: 0.7857\n",
      "Epoch 253/350\n",
      "38/62 [=================>............] - ETA: 0s - loss: 0.2099 - accuracy: 0.9263\n",
      "Epoch 253: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2037 - accuracy: 0.9251 - val_loss: 0.7596 - val_accuracy: 0.7857\n",
      "Epoch 254/350\n",
      "48/62 [======================>.......] - ETA: 0s - loss: 0.2119 - accuracy: 0.9208\n",
      "Epoch 254: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2031 - accuracy: 0.9251 - val_loss: 0.7595 - val_accuracy: 0.7857\n",
      "Epoch 255/350\n",
      "60/62 [============================>.] - ETA: 0s - loss: 0.2039 - accuracy: 0.9267\n",
      "Epoch 255: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2025 - accuracy: 0.9267 - val_loss: 0.7624 - val_accuracy: 0.7857\n",
      "Epoch 256/350\n",
      "48/62 [======================>.......] - ETA: 0s - loss: 0.2102 - accuracy: 0.9208\n",
      "Epoch 256: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2017 - accuracy: 0.9251 - val_loss: 0.7639 - val_accuracy: 0.7857\n",
      "Epoch 257/350\n",
      "42/62 [===================>..........] - ETA: 0s - loss: 0.2165 - accuracy: 0.9214\n",
      "Epoch 257: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2010 - accuracy: 0.9251 - val_loss: 0.7657 - val_accuracy: 0.7792\n",
      "Epoch 258/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.2061 - accuracy: 0.9240\n",
      "Epoch 258: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2003 - accuracy: 0.9251 - val_loss: 0.7673 - val_accuracy: 0.7792\n",
      "Epoch 259/350\n",
      "58/62 [===========================>..] - ETA: 0s - loss: 0.2039 - accuracy: 0.9276\n",
      "Epoch 259: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1995 - accuracy: 0.9300 - val_loss: 0.7688 - val_accuracy: 0.7792\n",
      "Epoch 260/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.2070 - accuracy: 0.9255\n",
      "Epoch 260: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1986 - accuracy: 0.9300 - val_loss: 0.7712 - val_accuracy: 0.7792\n",
      "Epoch 261/350\n",
      "62/62 [==============================] - ETA: 0s - loss: 0.1981 - accuracy: 0.9283\n",
      "Epoch 261: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1981 - accuracy: 0.9283 - val_loss: 0.7697 - val_accuracy: 0.7792\n",
      "Epoch 262/350\n",
      "45/62 [====================>.........] - ETA: 0s - loss: 0.2061 - accuracy: 0.9289\n",
      "Epoch 262: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1974 - accuracy: 0.9300 - val_loss: 0.7735 - val_accuracy: 0.7792\n",
      "Epoch 263/350\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.1977 - accuracy: 0.9279\n",
      "Epoch 263: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1967 - accuracy: 0.9283 - val_loss: 0.7751 - val_accuracy: 0.7792\n",
      "Epoch 264/350\n",
      "62/62 [==============================] - ETA: 0s - loss: 0.1959 - accuracy: 0.9283\n",
      "Epoch 264: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1959 - accuracy: 0.9283 - val_loss: 0.7768 - val_accuracy: 0.7792\n",
      "Epoch 265/350\n",
      "51/62 [=======================>......] - ETA: 0s - loss: 0.1988 - accuracy: 0.9275\n",
      "Epoch 265: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1951 - accuracy: 0.9283 - val_loss: 0.7798 - val_accuracy: 0.7792\n",
      "Epoch 266/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.2027 - accuracy: 0.9255\n",
      "Epoch 266: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1944 - accuracy: 0.9300 - val_loss: 0.7806 - val_accuracy: 0.7792\n",
      "Epoch 267/350\n",
      "41/62 [==================>...........] - ETA: 0s - loss: 0.2078 - accuracy: 0.9244\n",
      "Epoch 267: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.1939 - accuracy: 0.9300 - val_loss: 0.7823 - val_accuracy: 0.7792\n",
      "Epoch 268/350\n",
      "60/62 [============================>.] - ETA: 0s - loss: 0.1942 - accuracy: 0.9300\n",
      "Epoch 268: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1929 - accuracy: 0.9300 - val_loss: 0.7833 - val_accuracy: 0.7792\n",
      "Epoch 269/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.1977 - accuracy: 0.9280\n",
      "Epoch 269: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 1s 10ms/step - loss: 0.1923 - accuracy: 0.9300 - val_loss: 0.7819 - val_accuracy: 0.7792\n",
      "Epoch 270/350\n",
      "62/62 [==============================] - ETA: 0s - loss: 0.1915 - accuracy: 0.9300\n",
      "Epoch 270: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1915 - accuracy: 0.9300 - val_loss: 0.7864 - val_accuracy: 0.7727\n",
      "Epoch 271/350\n",
      "43/62 [===================>..........] - ETA: 0s - loss: 0.2021 - accuracy: 0.9279\n",
      "Epoch 271: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1906 - accuracy: 0.9316 - val_loss: 0.7888 - val_accuracy: 0.7727\n",
      "Epoch 272/350\n",
      "47/62 [=====================>........] - ETA: 0s - loss: 0.1958 - accuracy: 0.9277\n",
      "Epoch 272: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1904 - accuracy: 0.9300 - val_loss: 0.7915 - val_accuracy: 0.7792\n",
      "Epoch 273/350\n",
      "57/62 [==========================>...] - ETA: 0s - loss: 0.1946 - accuracy: 0.9263\n",
      "Epoch 273: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.1893 - accuracy: 0.9300 - val_loss: 0.7919 - val_accuracy: 0.7727\n",
      "Epoch 274/350\n",
      "57/62 [==========================>...] - ETA: 0s - loss: 0.1942 - accuracy: 0.9263\n",
      "Epoch 274: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.1888 - accuracy: 0.9300 - val_loss: 0.7950 - val_accuracy: 0.7662\n",
      "Epoch 275/350\n",
      "60/62 [============================>.] - ETA: 0s - loss: 0.1894 - accuracy: 0.9300\n",
      "Epoch 275: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1881 - accuracy: 0.9300 - val_loss: 0.7951 - val_accuracy: 0.7662\n",
      "Epoch 276/350\n",
      "57/62 [==========================>...] - ETA: 0s - loss: 0.1925 - accuracy: 0.9281\n",
      "Epoch 276: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1872 - accuracy: 0.9316 - val_loss: 0.7990 - val_accuracy: 0.7662\n",
      "Epoch 277/350\n",
      "47/62 [=====================>........] - ETA: 0s - loss: 0.1918 - accuracy: 0.9298\n",
      "Epoch 277: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.1867 - accuracy: 0.9316 - val_loss: 0.7996 - val_accuracy: 0.7662\n",
      "Epoch 278/350\n",
      "56/62 [==========================>...] - ETA: 0s - loss: 0.1912 - accuracy: 0.9304\n",
      "Epoch 278: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1857 - accuracy: 0.9332 - val_loss: 0.8055 - val_accuracy: 0.7597\n",
      "Epoch 279/350\n",
      "54/62 [=========================>....] - ETA: 0s - loss: 0.1913 - accuracy: 0.9259\n",
      "Epoch 279: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.1851 - accuracy: 0.9300 - val_loss: 0.8034 - val_accuracy: 0.7597\n",
      "Epoch 280/350\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.1878 - accuracy: 0.9322\n",
      "Epoch 280: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1845 - accuracy: 0.9332 - val_loss: 0.8055 - val_accuracy: 0.7662\n",
      "Epoch 281/350\n",
      "51/62 [=======================>......] - ETA: 0s - loss: 0.1871 - accuracy: 0.9294\n",
      "Epoch 281: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1838 - accuracy: 0.9300 - val_loss: 0.8099 - val_accuracy: 0.7597\n",
      "Epoch 282/350\n",
      "58/62 [===========================>..] - ETA: 0s - loss: 0.1874 - accuracy: 0.9328\n",
      "Epoch 282: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.1831 - accuracy: 0.9349 - val_loss: 0.8104 - val_accuracy: 0.7597\n",
      "Epoch 283/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.1855 - accuracy: 0.9302\n",
      "Epoch 283: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1824 - accuracy: 0.9316 - val_loss: 0.8103 - val_accuracy: 0.7597\n",
      "Epoch 284/350\n",
      "56/62 [==========================>...] - ETA: 0s - loss: 0.1873 - accuracy: 0.9304\n",
      "Epoch 284: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1820 - accuracy: 0.9332 - val_loss: 0.8138 - val_accuracy: 0.7597\n",
      "Epoch 285/350\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.1819 - accuracy: 0.9328\n",
      "Epoch 285: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.1809 - accuracy: 0.9332 - val_loss: 0.8165 - val_accuracy: 0.7662\n",
      "Epoch 286/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.1853 - accuracy: 0.9320\n",
      "Epoch 286: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.1804 - accuracy: 0.9332 - val_loss: 0.8167 - val_accuracy: 0.7597\n",
      "Epoch 287/350\n",
      "58/62 [===========================>..] - ETA: 0s - loss: 0.1841 - accuracy: 0.9310\n",
      "Epoch 287: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.1798 - accuracy: 0.9332 - val_loss: 0.8205 - val_accuracy: 0.7662\n",
      "Epoch 288/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.1817 - accuracy: 0.9321\n",
      "Epoch 288: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1791 - accuracy: 0.9332 - val_loss: 0.8202 - val_accuracy: 0.7662\n",
      "Epoch 289/350\n",
      "36/62 [================>.............] - ETA: 0s - loss: 0.1792 - accuracy: 0.9333\n",
      "Epoch 289: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1782 - accuracy: 0.9332 - val_loss: 0.8212 - val_accuracy: 0.7662\n",
      "Epoch 290/350\n",
      "45/62 [====================>.........] - ETA: 0s - loss: 0.1846 - accuracy: 0.9311\n",
      "Epoch 290: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.1779 - accuracy: 0.9349 - val_loss: 0.8260 - val_accuracy: 0.7662\n",
      "Epoch 291/350\n",
      "46/62 [=====================>........] - ETA: 0s - loss: 0.1805 - accuracy: 0.9304\n",
      "Epoch 291: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1769 - accuracy: 0.9332 - val_loss: 0.8288 - val_accuracy: 0.7662\n",
      "Epoch 292/350\n",
      "47/62 [=====================>........] - ETA: 0s - loss: 0.1810 - accuracy: 0.9340\n",
      "Epoch 292: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1764 - accuracy: 0.9365 - val_loss: 0.8306 - val_accuracy: 0.7662\n",
      "Epoch 293/350\n",
      "60/62 [============================>.] - ETA: 0s - loss: 0.1769 - accuracy: 0.9400\n",
      "Epoch 293: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1756 - accuracy: 0.9397 - val_loss: 0.8324 - val_accuracy: 0.7662\n",
      "Epoch 294/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.1799 - accuracy: 0.9360\n",
      "Epoch 294: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1751 - accuracy: 0.9365 - val_loss: 0.8347 - val_accuracy: 0.7662\n",
      "Epoch 295/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.1790 - accuracy: 0.9380\n",
      "Epoch 295: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.1739 - accuracy: 0.9381 - val_loss: 0.8365 - val_accuracy: 0.7662\n",
      "Epoch 296/350\n",
      "46/62 [=====================>........] - ETA: 0s - loss: 0.1770 - accuracy: 0.9348\n",
      "Epoch 296: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1734 - accuracy: 0.9365 - val_loss: 0.8389 - val_accuracy: 0.7662\n",
      "Epoch 297/350\n",
      "48/62 [======================>.......] - ETA: 0s - loss: 0.1799 - accuracy: 0.9375\n",
      "Epoch 297: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1729 - accuracy: 0.9397 - val_loss: 0.8394 - val_accuracy: 0.7662\n",
      "Epoch 298/350\n",
      "43/62 [===================>..........] - ETA: 0s - loss: 0.1818 - accuracy: 0.9349\n",
      "Epoch 298: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1722 - accuracy: 0.9381 - val_loss: 0.8414 - val_accuracy: 0.7662\n",
      "Epoch 299/350\n",
      "38/62 [=================>............] - ETA: 0s - loss: 0.1745 - accuracy: 0.9395\n",
      "Epoch 299: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.1714 - accuracy: 0.9381 - val_loss: 0.8431 - val_accuracy: 0.7662\n",
      "Epoch 300/350\n",
      "43/62 [===================>..........] - ETA: 0s - loss: 0.1803 - accuracy: 0.9395\n",
      "Epoch 300: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1709 - accuracy: 0.9414 - val_loss: 0.8449 - val_accuracy: 0.7662\n",
      "Epoch 301/350\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.1731 - accuracy: 0.9424\n",
      "Epoch 301: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1699 - accuracy: 0.9430 - val_loss: 0.8462 - val_accuracy: 0.7662\n",
      "Epoch 302/350\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.1702 - accuracy: 0.9426\n",
      "Epoch 302: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1693 - accuracy: 0.9430 - val_loss: 0.8492 - val_accuracy: 0.7662\n",
      "Epoch 303/350\n",
      "44/62 [====================>.........] - ETA: 0s - loss: 0.1746 - accuracy: 0.9432\n",
      "Epoch 303: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1683 - accuracy: 0.9446 - val_loss: 0.8534 - val_accuracy: 0.7662\n",
      "Epoch 304/350\n",
      "41/62 [==================>...........] - ETA: 0s - loss: 0.1785 - accuracy: 0.9415\n",
      "Epoch 304: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1677 - accuracy: 0.9430 - val_loss: 0.8547 - val_accuracy: 0.7727\n",
      "Epoch 305/350\n",
      "57/62 [==========================>...] - ETA: 0s - loss: 0.1712 - accuracy: 0.9386\n",
      "Epoch 305: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1666 - accuracy: 0.9414 - val_loss: 0.8585 - val_accuracy: 0.7727\n",
      "Epoch 306/350\n",
      "44/62 [====================>.........] - ETA: 0s - loss: 0.1718 - accuracy: 0.9432\n",
      "Epoch 306: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.1660 - accuracy: 0.9430 - val_loss: 0.8595 - val_accuracy: 0.7727\n",
      "Epoch 307/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.1694 - accuracy: 0.9440\n",
      "Epoch 307: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1653 - accuracy: 0.9446 - val_loss: 0.8637 - val_accuracy: 0.7662\n",
      "Epoch 308/350\n",
      "62/62 [==============================] - ETA: 0s - loss: 0.1648 - accuracy: 0.9446\n",
      "Epoch 308: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1648 - accuracy: 0.9446 - val_loss: 0.8640 - val_accuracy: 0.7727\n",
      "Epoch 309/350\n",
      "48/62 [======================>.......] - ETA: 0s - loss: 0.1702 - accuracy: 0.9438\n",
      "Epoch 309: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1642 - accuracy: 0.9463 - val_loss: 0.8692 - val_accuracy: 0.7727\n",
      "Epoch 310/350\n",
      "52/62 [========================>.....] - ETA: 0s - loss: 0.1659 - accuracy: 0.9462\n",
      "Epoch 310: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.1634 - accuracy: 0.9463 - val_loss: 0.8720 - val_accuracy: 0.7727\n",
      "Epoch 311/350\n",
      "44/62 [====================>.........] - ETA: 0s - loss: 0.1683 - accuracy: 0.9455\n",
      "Epoch 311: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1627 - accuracy: 0.9446 - val_loss: 0.8729 - val_accuracy: 0.7727\n",
      "Epoch 312/350\n",
      "46/62 [=====================>........] - ETA: 0s - loss: 0.1640 - accuracy: 0.9457\n",
      "Epoch 312: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.1619 - accuracy: 0.9463 - val_loss: 0.8742 - val_accuracy: 0.7727\n",
      "Epoch 313/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.1685 - accuracy: 0.9400\n",
      "Epoch 313: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1614 - accuracy: 0.9446 - val_loss: 0.8776 - val_accuracy: 0.7727\n",
      "Epoch 314/350\n",
      "46/62 [=====================>........] - ETA: 0s - loss: 0.1628 - accuracy: 0.9457\n",
      "Epoch 314: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1607 - accuracy: 0.9463 - val_loss: 0.8823 - val_accuracy: 0.7662\n",
      "Epoch 315/350\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.1608 - accuracy: 0.9475\n",
      "Epoch 315: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 0.1599 - accuracy: 0.9479 - val_loss: 0.8808 - val_accuracy: 0.7727\n",
      "Epoch 316/350\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.1604 - accuracy: 0.9459\n",
      "Epoch 316: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1595 - accuracy: 0.9463 - val_loss: 0.8868 - val_accuracy: 0.7662\n",
      "Epoch 317/350\n",
      "44/62 [====================>.........] - ETA: 0s - loss: 0.1639 - accuracy: 0.9523\n",
      "Epoch 317: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1588 - accuracy: 0.9511 - val_loss: 0.8875 - val_accuracy: 0.7662\n",
      "Epoch 318/350\n",
      "58/62 [===========================>..] - ETA: 0s - loss: 0.1617 - accuracy: 0.9466\n",
      "Epoch 318: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1578 - accuracy: 0.9495 - val_loss: 0.8870 - val_accuracy: 0.7662\n",
      "Epoch 319/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.1643 - accuracy: 0.9455\n",
      "Epoch 319: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.1574 - accuracy: 0.9495 - val_loss: 0.8911 - val_accuracy: 0.7662\n",
      "Epoch 320/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.1598 - accuracy: 0.9500\n",
      "Epoch 320: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.1564 - accuracy: 0.9495 - val_loss: 0.8903 - val_accuracy: 0.7662\n",
      "Epoch 321/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.1599 - accuracy: 0.9520\n",
      "Epoch 321: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.1563 - accuracy: 0.9511 - val_loss: 0.8949 - val_accuracy: 0.7662\n",
      "Epoch 322/350\n",
      "40/62 [==================>...........] - ETA: 0s - loss: 0.1593 - accuracy: 0.9525\n",
      "Epoch 322: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1552 - accuracy: 0.9511 - val_loss: 0.8982 - val_accuracy: 0.7662\n",
      "Epoch 323/350\n",
      "46/62 [=====================>........] - ETA: 0s - loss: 0.1566 - accuracy: 0.9522\n",
      "Epoch 323: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1542 - accuracy: 0.9511 - val_loss: 0.9037 - val_accuracy: 0.7662\n",
      "Epoch 324/350\n",
      "58/62 [===========================>..] - ETA: 0s - loss: 0.1577 - accuracy: 0.9483\n",
      "Epoch 324: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 0.1538 - accuracy: 0.9511 - val_loss: 0.9036 - val_accuracy: 0.7662\n",
      "Epoch 325/350\n",
      "52/62 [========================>.....] - ETA: 0s - loss: 0.1553 - accuracy: 0.9519\n",
      "Epoch 325: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1533 - accuracy: 0.9511 - val_loss: 0.9073 - val_accuracy: 0.7662\n",
      "Epoch 326/350\n",
      "44/62 [====================>.........] - ETA: 0s - loss: 0.1570 - accuracy: 0.9523\n",
      "Epoch 326: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1523 - accuracy: 0.9511 - val_loss: 0.9125 - val_accuracy: 0.7662\n",
      "Epoch 327/350\n",
      "48/62 [======================>.......] - ETA: 0s - loss: 0.1571 - accuracy: 0.9500\n",
      "Epoch 327: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1516 - accuracy: 0.9511 - val_loss: 0.9105 - val_accuracy: 0.7662\n",
      "Epoch 328/350\n",
      "50/62 [=======================>......] - ETA: 0s - loss: 0.1547 - accuracy: 0.9520\n",
      "Epoch 328: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.1514 - accuracy: 0.9511 - val_loss: 0.9133 - val_accuracy: 0.7662\n",
      "Epoch 329/350\n",
      "58/62 [===========================>..] - ETA: 0s - loss: 0.1537 - accuracy: 0.9483\n",
      "Epoch 329: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1500 - accuracy: 0.9511 - val_loss: 0.9185 - val_accuracy: 0.7662\n",
      "Epoch 330/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.1512 - accuracy: 0.9528\n",
      "Epoch 330: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1495 - accuracy: 0.9528 - val_loss: 0.9170 - val_accuracy: 0.7662\n",
      "Epoch 331/350\n",
      "55/62 [=========================>....] - ETA: 0s - loss: 0.1560 - accuracy: 0.9491\n",
      "Epoch 331: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1494 - accuracy: 0.9528 - val_loss: 0.9220 - val_accuracy: 0.7662\n",
      "Epoch 332/350\n",
      "42/62 [===================>..........] - ETA: 0s - loss: 0.1572 - accuracy: 0.9548\n",
      "Epoch 332: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1481 - accuracy: 0.9544 - val_loss: 0.9265 - val_accuracy: 0.7662\n",
      "Epoch 333/350\n",
      "58/62 [===========================>..] - ETA: 0s - loss: 0.1514 - accuracy: 0.9534\n",
      "Epoch 333: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.1477 - accuracy: 0.9560 - val_loss: 0.9266 - val_accuracy: 0.7662\n",
      "Epoch 334/350\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.1497 - accuracy: 0.9542\n",
      "Epoch 334: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.1470 - accuracy: 0.9560 - val_loss: 0.9306 - val_accuracy: 0.7662\n",
      "Epoch 335/350\n",
      "62/62 [==============================] - ETA: 0s - loss: 0.1464 - accuracy: 0.9560\n",
      "Epoch 335: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1464 - accuracy: 0.9560 - val_loss: 0.9318 - val_accuracy: 0.7662\n",
      "Epoch 336/350\n",
      "57/62 [==========================>...] - ETA: 0s - loss: 0.1501 - accuracy: 0.9526\n",
      "Epoch 336: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.1460 - accuracy: 0.9560 - val_loss: 0.9378 - val_accuracy: 0.7662\n",
      "Epoch 337/350\n",
      "44/62 [====================>.........] - ETA: 0s - loss: 0.1497 - accuracy: 0.9591\n",
      "Epoch 337: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.1451 - accuracy: 0.9560 - val_loss: 0.9391 - val_accuracy: 0.7662\n",
      "Epoch 338/350\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.1473 - accuracy: 0.9542\n",
      "Epoch 338: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1447 - accuracy: 0.9560 - val_loss: 0.9425 - val_accuracy: 0.7662\n",
      "Epoch 339/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.1452 - accuracy: 0.9566\n",
      "Epoch 339: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.1438 - accuracy: 0.9560 - val_loss: 0.9386 - val_accuracy: 0.7662\n",
      "Epoch 340/350\n",
      "48/62 [======================>.......] - ETA: 0s - loss: 0.1480 - accuracy: 0.9563\n",
      "Epoch 340: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.1430 - accuracy: 0.9560 - val_loss: 0.9472 - val_accuracy: 0.7662\n",
      "Epoch 341/350\n",
      "49/62 [======================>.......] - ETA: 0s - loss: 0.1464 - accuracy: 0.9571\n",
      "Epoch 341: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.1427 - accuracy: 0.9560 - val_loss: 0.9492 - val_accuracy: 0.7662\n",
      "Epoch 342/350\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.1428 - accuracy: 0.9557\n",
      "Epoch 342: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.1419 - accuracy: 0.9560 - val_loss: 0.9529 - val_accuracy: 0.7662\n",
      "Epoch 343/350\n",
      "53/62 [========================>.....] - ETA: 0s - loss: 0.1425 - accuracy: 0.9566\n",
      "Epoch 343: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.1413 - accuracy: 0.9560 - val_loss: 0.9524 - val_accuracy: 0.7662\n",
      "Epoch 344/350\n",
      "47/62 [=====================>........] - ETA: 0s - loss: 0.1432 - accuracy: 0.9574\n",
      "Epoch 344: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1405 - accuracy: 0.9560 - val_loss: 0.9570 - val_accuracy: 0.7662\n",
      "Epoch 345/350\n",
      "49/62 [======================>.......] - ETA: 0s - loss: 0.1440 - accuracy: 0.9571\n",
      "Epoch 345: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.1403 - accuracy: 0.9560 - val_loss: 0.9583 - val_accuracy: 0.7662\n",
      "Epoch 346/350\n",
      "44/62 [====================>.........] - ETA: 0s - loss: 0.1436 - accuracy: 0.9591\n",
      "Epoch 346: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1393 - accuracy: 0.9560 - val_loss: 0.9616 - val_accuracy: 0.7662\n",
      "Epoch 347/350\n",
      "49/62 [======================>.......] - ETA: 0s - loss: 0.1424 - accuracy: 0.9571\n",
      "Epoch 347: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.1388 - accuracy: 0.9560 - val_loss: 0.9647 - val_accuracy: 0.7662\n",
      "Epoch 348/350\n",
      "44/62 [====================>.........] - ETA: 0s - loss: 0.1426 - accuracy: 0.9591\n",
      "Epoch 348: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.1383 - accuracy: 0.9560 - val_loss: 0.9677 - val_accuracy: 0.7662\n",
      "Epoch 349/350\n",
      "49/62 [======================>.......] - ETA: 0s - loss: 0.1412 - accuracy: 0.9571\n",
      "Epoch 349: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1378 - accuracy: 0.9560 - val_loss: 0.9713 - val_accuracy: 0.7662\n",
      "Epoch 350/350\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.1376 - accuracy: 0.9557\n",
      "Epoch 350: val_loss did not improve from 0.32289\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.1368 - accuracy: 0.9560 - val_loss: 0.9737 - val_accuracy: 0.7662\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# en reprenant les x_train, X_val, y_train, y_val précédents\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "X_val_scaled = scaler.fit_transform(x_val)\n",
    "\n",
    "model = my_model((8,))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# entraînement sur le dataset standardisé\n",
    "history=model.fit(x_train_scaled, y_train, epochs=350, batch_size=10, shuffle=False, validation_data=(X_val_scaled,y_val), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "classe 1 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std  = np.sqrt(scaler.var_)\n",
    "mean = scaler.mean_\n",
    "X=np.array([[6.,148.,72.,35.,0.,33.6,0.627,50.]])\n",
    "X_1_scaled = (X_1 - mean) / std\n",
    "\n",
    "res1=model.predict([X_1_scaled])\n",
    "res2=model.predict([X])\n",
    "print('classe {0} '.format(round(res1[0,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris.keys()\n",
    "\n",
    "x = iris[\"data\"]\n",
    "\n",
    "# On créé une liste qui contient 1 lorsque la fleur est de type 2 et 0 sinon pour faire une classification\n",
    "y = iris[\"target\"]\n",
    "\n",
    "y_cat = tf.keras.utils.to_categorical(y, 3)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y_cat, test_size=0.2, random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_model(in_shape,out_shape=1):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_shape=in_shape,activation='relu'))\n",
    "    model.add(Dense(16, input_shape=(8,),activation='relu'))\n",
    "    model.add(Dense(out_shape, input_shape=(16,),activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "12/12 [==============================] - 1s 30ms/step - loss: 0.9919 - accuracy: 0.3500 - val_loss: 1.0605 - val_accuracy: 0.2667\n",
      "Epoch 2/150\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.8955 - accuracy: 0.3500 - val_loss: 0.9590 - val_accuracy: 0.2667\n",
      "Epoch 3/150\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.8249 - accuracy: 0.3500 - val_loss: 0.8821 - val_accuracy: 0.2667\n",
      "Epoch 4/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.7731 - accuracy: 0.3500 - val_loss: 0.8198 - val_accuracy: 0.2667\n",
      "Epoch 5/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.7295 - accuracy: 0.3500 - val_loss: 0.7702 - val_accuracy: 0.2667\n",
      "Epoch 6/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6923 - accuracy: 0.3500 - val_loss: 0.7299 - val_accuracy: 0.2667\n",
      "Epoch 7/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6629 - accuracy: 0.3500 - val_loss: 0.6931 - val_accuracy: 0.2667\n",
      "Epoch 8/150\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.6334 - accuracy: 0.3500 - val_loss: 0.6644 - val_accuracy: 0.2667\n",
      "Epoch 9/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6103 - accuracy: 0.3500 - val_loss: 0.6366 - val_accuracy: 0.2667\n",
      "Epoch 10/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5887 - accuracy: 0.3500 - val_loss: 0.6131 - val_accuracy: 0.2667\n",
      "Epoch 11/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5706 - accuracy: 0.3500 - val_loss: 0.5910 - val_accuracy: 0.2667\n",
      "Epoch 12/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5532 - accuracy: 0.5417 - val_loss: 0.5707 - val_accuracy: 0.7000\n",
      "Epoch 13/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5376 - accuracy: 0.6417 - val_loss: 0.5521 - val_accuracy: 0.7000\n",
      "Epoch 14/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5237 - accuracy: 0.6500 - val_loss: 0.5331 - val_accuracy: 0.7333\n",
      "Epoch 15/150\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.5105 - accuracy: 0.6500 - val_loss: 0.5161 - val_accuracy: 0.7333\n",
      "Epoch 16/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4975 - accuracy: 0.6500 - val_loss: 0.5022 - val_accuracy: 0.7333\n",
      "Epoch 17/150\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.4853 - accuracy: 0.6500 - val_loss: 0.4865 - val_accuracy: 0.7333\n",
      "Epoch 18/150\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.4740 - accuracy: 0.6500 - val_loss: 0.4708 - val_accuracy: 0.7333\n",
      "Epoch 19/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4624 - accuracy: 0.6500 - val_loss: 0.4569 - val_accuracy: 0.7333\n",
      "Epoch 20/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4505 - accuracy: 0.6500 - val_loss: 0.4433 - val_accuracy: 0.7333\n",
      "Epoch 21/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4381 - accuracy: 0.6500 - val_loss: 0.4270 - val_accuracy: 0.7333\n",
      "Epoch 22/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4250 - accuracy: 0.6500 - val_loss: 0.4130 - val_accuracy: 0.7333\n",
      "Epoch 23/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4133 - accuracy: 0.6500 - val_loss: 0.4005 - val_accuracy: 0.7333\n",
      "Epoch 24/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4032 - accuracy: 0.6667 - val_loss: 0.3875 - val_accuracy: 0.7333\n",
      "Epoch 25/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3940 - accuracy: 0.6667 - val_loss: 0.3742 - val_accuracy: 0.7333\n",
      "Epoch 26/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3846 - accuracy: 0.6750 - val_loss: 0.3629 - val_accuracy: 0.7333\n",
      "Epoch 27/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3760 - accuracy: 0.6917 - val_loss: 0.3516 - val_accuracy: 0.7333\n",
      "Epoch 28/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3671 - accuracy: 0.6833 - val_loss: 0.3421 - val_accuracy: 0.7333\n",
      "Epoch 29/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3595 - accuracy: 0.7083 - val_loss: 0.3307 - val_accuracy: 0.7667\n",
      "Epoch 30/150\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.3508 - accuracy: 0.7500 - val_loss: 0.3219 - val_accuracy: 0.7333\n",
      "Epoch 31/150\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.3438 - accuracy: 0.7333 - val_loss: 0.3140 - val_accuracy: 0.7333\n",
      "Epoch 32/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3373 - accuracy: 0.7750 - val_loss: 0.3043 - val_accuracy: 0.8000\n",
      "Epoch 33/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3300 - accuracy: 0.8250 - val_loss: 0.2969 - val_accuracy: 0.8333\n",
      "Epoch 34/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3238 - accuracy: 0.8167 - val_loss: 0.2905 - val_accuracy: 0.8000\n",
      "Epoch 35/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3181 - accuracy: 0.8250 - val_loss: 0.2826 - val_accuracy: 0.8333\n",
      "Epoch 36/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3124 - accuracy: 0.8667 - val_loss: 0.2758 - val_accuracy: 0.8333\n",
      "Epoch 37/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3077 - accuracy: 0.8333 - val_loss: 0.2709 - val_accuracy: 0.8333\n",
      "Epoch 38/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3027 - accuracy: 0.8583 - val_loss: 0.2635 - val_accuracy: 0.8667\n",
      "Epoch 39/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2964 - accuracy: 0.8750 - val_loss: 0.2585 - val_accuracy: 0.8333\n",
      "Epoch 40/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2917 - accuracy: 0.8750 - val_loss: 0.2539 - val_accuracy: 0.8667\n",
      "Epoch 41/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2870 - accuracy: 0.8833 - val_loss: 0.2484 - val_accuracy: 0.8667\n",
      "Epoch 42/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2822 - accuracy: 0.8917 - val_loss: 0.2435 - val_accuracy: 0.9333\n",
      "Epoch 43/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2778 - accuracy: 0.9083 - val_loss: 0.2389 - val_accuracy: 0.9333\n",
      "Epoch 44/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2741 - accuracy: 0.8917 - val_loss: 0.2359 - val_accuracy: 0.9000\n",
      "Epoch 45/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2694 - accuracy: 0.8917 - val_loss: 0.2310 - val_accuracy: 0.9333\n",
      "Epoch 46/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2657 - accuracy: 0.9417 - val_loss: 0.2264 - val_accuracy: 0.9333\n",
      "Epoch 47/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2617 - accuracy: 0.9500 - val_loss: 0.2227 - val_accuracy: 0.9333\n",
      "Epoch 48/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2573 - accuracy: 0.9167 - val_loss: 0.2203 - val_accuracy: 0.9000\n",
      "Epoch 49/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2536 - accuracy: 0.9167 - val_loss: 0.2144 - val_accuracy: 0.9333\n",
      "Epoch 50/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2501 - accuracy: 0.9583 - val_loss: 0.2107 - val_accuracy: 0.9333\n",
      "Epoch 51/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2463 - accuracy: 0.9167 - val_loss: 0.2091 - val_accuracy: 0.9333\n",
      "Epoch 52/150\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.2411 - accuracy: 0.9417 - val_loss: 0.2032 - val_accuracy: 0.9333\n",
      "Epoch 53/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2382 - accuracy: 0.9667 - val_loss: 0.2003 - val_accuracy: 0.9333\n",
      "Epoch 54/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2339 - accuracy: 0.9583 - val_loss: 0.1967 - val_accuracy: 0.9333\n",
      "Epoch 55/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2321 - accuracy: 0.9250 - val_loss: 0.1946 - val_accuracy: 0.9333\n",
      "Epoch 56/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2265 - accuracy: 0.9750 - val_loss: 0.1893 - val_accuracy: 1.0000\n",
      "Epoch 57/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2233 - accuracy: 0.9750 - val_loss: 0.1868 - val_accuracy: 0.9667\n",
      "Epoch 58/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2191 - accuracy: 0.9500 - val_loss: 0.1861 - val_accuracy: 0.9333\n",
      "Epoch 59/150\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.2151 - accuracy: 0.9667 - val_loss: 0.1803 - val_accuracy: 0.9667\n",
      "Epoch 60/150\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.2115 - accuracy: 0.9667 - val_loss: 0.1768 - val_accuracy: 1.0000\n",
      "Epoch 61/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2091 - accuracy: 0.9667 - val_loss: 0.1736 - val_accuracy: 1.0000\n",
      "Epoch 62/150\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.2049 - accuracy: 0.9667 - val_loss: 0.1721 - val_accuracy: 0.9667\n",
      "Epoch 63/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2005 - accuracy: 0.9667 - val_loss: 0.1675 - val_accuracy: 1.0000\n",
      "Epoch 64/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1974 - accuracy: 0.9667 - val_loss: 0.1646 - val_accuracy: 1.0000\n",
      "Epoch 65/150\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1953 - accuracy: 0.9667 - val_loss: 0.1642 - val_accuracy: 0.9667\n",
      "Epoch 66/150\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1908 - accuracy: 0.9667 - val_loss: 0.1588 - val_accuracy: 1.0000\n",
      "Epoch 67/150\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.1873 - accuracy: 0.9667 - val_loss: 0.1566 - val_accuracy: 1.0000\n",
      "Epoch 68/150\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.1843 - accuracy: 0.9750 - val_loss: 0.1537 - val_accuracy: 1.0000\n",
      "Epoch 69/150\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.1813 - accuracy: 0.9750 - val_loss: 0.1513 - val_accuracy: 1.0000\n",
      "Epoch 70/150\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1779 - accuracy: 0.9750 - val_loss: 0.1491 - val_accuracy: 1.0000\n",
      "Epoch 71/150\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1765 - accuracy: 0.9667 - val_loss: 0.1475 - val_accuracy: 0.9667\n",
      "Epoch 72/150\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1733 - accuracy: 0.9667 - val_loss: 0.1425 - val_accuracy: 1.0000\n",
      "Epoch 73/150\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.1690 - accuracy: 0.9750 - val_loss: 0.1406 - val_accuracy: 1.0000\n",
      "Epoch 74/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1663 - accuracy: 0.9667 - val_loss: 0.1388 - val_accuracy: 1.0000\n",
      "Epoch 75/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.1645 - accuracy: 0.9750 - val_loss: 0.1355 - val_accuracy: 1.0000\n",
      "Epoch 76/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1609 - accuracy: 0.9750 - val_loss: 0.1337 - val_accuracy: 1.0000\n",
      "Epoch 77/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1577 - accuracy: 0.9667 - val_loss: 0.1320 - val_accuracy: 1.0000\n",
      "Epoch 78/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1546 - accuracy: 0.9750 - val_loss: 0.1289 - val_accuracy: 1.0000\n",
      "Epoch 79/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1521 - accuracy: 0.9750 - val_loss: 0.1260 - val_accuracy: 1.0000\n",
      "Epoch 80/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1518 - accuracy: 0.9750 - val_loss: 0.1245 - val_accuracy: 1.0000\n",
      "Epoch 81/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1469 - accuracy: 0.9750 - val_loss: 0.1223 - val_accuracy: 1.0000\n",
      "Epoch 82/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1450 - accuracy: 0.9750 - val_loss: 0.1203 - val_accuracy: 1.0000\n",
      "Epoch 83/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1427 - accuracy: 0.9750 - val_loss: 0.1178 - val_accuracy: 1.0000\n",
      "Epoch 84/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1402 - accuracy: 0.9750 - val_loss: 0.1158 - val_accuracy: 1.0000\n",
      "Epoch 85/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1379 - accuracy: 0.9750 - val_loss: 0.1147 - val_accuracy: 1.0000\n",
      "Epoch 86/150\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.1366 - accuracy: 0.9667 - val_loss: 0.1125 - val_accuracy: 1.0000\n",
      "Epoch 87/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1332 - accuracy: 0.9750 - val_loss: 0.1100 - val_accuracy: 1.0000\n",
      "Epoch 88/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1317 - accuracy: 0.9750 - val_loss: 0.1083 - val_accuracy: 1.0000\n",
      "Epoch 89/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1293 - accuracy: 0.9750 - val_loss: 0.1067 - val_accuracy: 1.0000\n",
      "Epoch 90/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1273 - accuracy: 0.9750 - val_loss: 0.1055 - val_accuracy: 1.0000\n",
      "Epoch 91/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1266 - accuracy: 0.9750 - val_loss: 0.1034 - val_accuracy: 1.0000\n",
      "Epoch 92/150\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.1237 - accuracy: 0.9750 - val_loss: 0.1013 - val_accuracy: 1.0000\n",
      "Epoch 93/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1223 - accuracy: 0.9750 - val_loss: 0.1003 - val_accuracy: 1.0000\n",
      "Epoch 94/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1204 - accuracy: 0.9750 - val_loss: 0.0994 - val_accuracy: 1.0000\n",
      "Epoch 95/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1182 - accuracy: 0.9750 - val_loss: 0.0972 - val_accuracy: 1.0000\n",
      "Epoch 96/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1168 - accuracy: 0.9750 - val_loss: 0.0956 - val_accuracy: 1.0000\n",
      "Epoch 97/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1149 - accuracy: 0.9750 - val_loss: 0.0950 - val_accuracy: 1.0000\n",
      "Epoch 98/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1138 - accuracy: 0.9667 - val_loss: 0.0932 - val_accuracy: 1.0000\n",
      "Epoch 99/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1137 - accuracy: 0.9750 - val_loss: 0.0914 - val_accuracy: 1.0000\n",
      "Epoch 100/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1105 - accuracy: 0.9750 - val_loss: 0.0907 - val_accuracy: 1.0000\n",
      "Epoch 101/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1095 - accuracy: 0.9667 - val_loss: 0.0895 - val_accuracy: 1.0000\n",
      "Epoch 102/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1087 - accuracy: 0.9750 - val_loss: 0.0874 - val_accuracy: 1.0000\n",
      "Epoch 103/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1060 - accuracy: 0.9750 - val_loss: 0.0864 - val_accuracy: 1.0000\n",
      "Epoch 104/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1052 - accuracy: 0.9750 - val_loss: 0.0851 - val_accuracy: 1.0000\n",
      "Epoch 105/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1035 - accuracy: 0.9750 - val_loss: 0.0849 - val_accuracy: 1.0000\n",
      "Epoch 106/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1029 - accuracy: 0.9750 - val_loss: 0.0840 - val_accuracy: 1.0000\n",
      "Epoch 107/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1010 - accuracy: 0.9750 - val_loss: 0.0816 - val_accuracy: 1.0000\n",
      "Epoch 108/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1002 - accuracy: 0.9750 - val_loss: 0.0806 - val_accuracy: 1.0000\n",
      "Epoch 109/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1005 - accuracy: 0.9750 - val_loss: 0.0799 - val_accuracy: 1.0000\n",
      "Epoch 110/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0977 - accuracy: 0.9750 - val_loss: 0.0788 - val_accuracy: 1.0000\n",
      "Epoch 111/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0968 - accuracy: 0.9750 - val_loss: 0.0779 - val_accuracy: 1.0000\n",
      "Epoch 112/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0954 - accuracy: 0.9750 - val_loss: 0.0767 - val_accuracy: 1.0000\n",
      "Epoch 113/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0948 - accuracy: 0.9750 - val_loss: 0.0759 - val_accuracy: 1.0000\n",
      "Epoch 114/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0956 - accuracy: 0.9750 - val_loss: 0.0747 - val_accuracy: 1.0000\n",
      "Epoch 115/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0917 - accuracy: 0.9750 - val_loss: 0.0742 - val_accuracy: 1.0000\n",
      "Epoch 116/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0917 - accuracy: 0.9750 - val_loss: 0.0731 - val_accuracy: 1.0000\n",
      "Epoch 117/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0934 - accuracy: 0.9667 - val_loss: 0.0729 - val_accuracy: 1.0000\n",
      "Epoch 118/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0887 - accuracy: 0.9750 - val_loss: 0.0715 - val_accuracy: 1.0000\n",
      "Epoch 119/150\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0894 - accuracy: 0.9750 - val_loss: 0.0709 - val_accuracy: 1.0000\n",
      "Epoch 120/150\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0889 - accuracy: 0.9750 - val_loss: 0.0699 - val_accuracy: 1.0000\n",
      "Epoch 121/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0870 - accuracy: 0.9750 - val_loss: 0.0689 - val_accuracy: 1.0000\n",
      "Epoch 122/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0863 - accuracy: 0.9750 - val_loss: 0.0682 - val_accuracy: 1.0000\n",
      "Epoch 123/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0865 - accuracy: 0.9750 - val_loss: 0.0675 - val_accuracy: 1.0000\n",
      "Epoch 124/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0855 - accuracy: 0.9750 - val_loss: 0.0671 - val_accuracy: 1.0000\n",
      "Epoch 125/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0862 - accuracy: 0.9750 - val_loss: 0.0662 - val_accuracy: 1.0000\n",
      "Epoch 126/150\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0847 - accuracy: 0.9750 - val_loss: 0.0658 - val_accuracy: 1.0000\n",
      "Epoch 127/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0841 - accuracy: 0.9750 - val_loss: 0.0649 - val_accuracy: 1.0000\n",
      "Epoch 128/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0836 - accuracy: 0.9750 - val_loss: 0.0644 - val_accuracy: 1.0000\n",
      "Epoch 129/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0810 - accuracy: 0.9750 - val_loss: 0.0637 - val_accuracy: 1.0000\n",
      "Epoch 130/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0811 - accuracy: 0.9750 - val_loss: 0.0627 - val_accuracy: 1.0000\n",
      "Epoch 131/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0800 - accuracy: 0.9750 - val_loss: 0.0621 - val_accuracy: 1.0000\n",
      "Epoch 132/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0795 - accuracy: 0.9750 - val_loss: 0.0620 - val_accuracy: 1.0000\n",
      "Epoch 133/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0783 - accuracy: 0.9750 - val_loss: 0.0609 - val_accuracy: 1.0000\n",
      "Epoch 134/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0777 - accuracy: 0.9750 - val_loss: 0.0604 - val_accuracy: 1.0000\n",
      "Epoch 135/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0785 - accuracy: 0.9750 - val_loss: 0.0600 - val_accuracy: 1.0000\n",
      "Epoch 136/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0767 - accuracy: 0.9750 - val_loss: 0.0593 - val_accuracy: 1.0000\n",
      "Epoch 137/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0764 - accuracy: 0.9750 - val_loss: 0.0587 - val_accuracy: 1.0000\n",
      "Epoch 138/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0756 - accuracy: 0.9750 - val_loss: 0.0581 - val_accuracy: 1.0000\n",
      "Epoch 139/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0758 - accuracy: 0.9750 - val_loss: 0.0579 - val_accuracy: 1.0000\n",
      "Epoch 140/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0742 - accuracy: 0.9750 - val_loss: 0.0571 - val_accuracy: 1.0000\n",
      "Epoch 141/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0738 - accuracy: 0.9750 - val_loss: 0.0566 - val_accuracy: 1.0000\n",
      "Epoch 142/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0739 - accuracy: 0.9750 - val_loss: 0.0560 - val_accuracy: 1.0000\n",
      "Epoch 143/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0736 - accuracy: 0.9750 - val_loss: 0.0557 - val_accuracy: 1.0000\n",
      "Epoch 144/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0723 - accuracy: 0.9750 - val_loss: 0.0553 - val_accuracy: 1.0000\n",
      "Epoch 145/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0720 - accuracy: 0.9750 - val_loss: 0.0547 - val_accuracy: 1.0000\n",
      "Epoch 146/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0714 - accuracy: 0.9750 - val_loss: 0.0543 - val_accuracy: 1.0000\n",
      "Epoch 147/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0725 - accuracy: 0.9750 - val_loss: 0.0541 - val_accuracy: 1.0000\n",
      "Epoch 148/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0706 - accuracy: 0.9750 - val_loss: 0.0534 - val_accuracy: 1.0000\n",
      "Epoch 149/150\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0710 - accuracy: 0.9750 - val_loss: 0.0529 - val_accuracy: 1.0000\n",
      "Epoch 150/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0706 - accuracy: 0.9750 - val_loss: 0.0538 - val_accuracy: 1.0000\n",
      "time 10.779576301574707\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0698 - accuracy: 0.9750\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0538 - accuracy: 1.0000\n",
      "train_accuracy : 0.98\n",
      "val_accuracy : 1.00\n"
     ]
    }
   ],
   "source": [
    "model = my_model((4,),out_shape=3)\n",
    "model.compile(loss='binary_crossentropy', optimizer=tf.optimizers.Adam(learning_rate=1e-3), metrics=['accuracy'])\n",
    "tic = time.time()\n",
    "history = model.fit(x_train, y_train,validation_data=(x_val,y_val),epochs=150, batch_size=10,verbose=1)\n",
    "tac = time.time()\n",
    "print(f\"time {tac-tic}\")\n",
    "# _, accuracy = model.evaluate(X, y,verbose=1)\n",
    "_, train_accuracy = model.evaluate(x_train, y_train,verbose=1)\n",
    "_, val_accuracy = model.evaluate(x_val, y_val,verbose=1)\n",
    "print(\"train_accuracy : {:.2f}\".format(train_accuracy))\n",
    "print(\"val_accuracy : {:.2f}\".format(val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120,)\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(model.predict(x_train),axis=1)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.datasets.mnist as mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_190 (Dense)           (None, 64)                50240     \n",
      "                                                                 \n",
      " dense_191 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_192 (Dense)           (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52650 (205.66 KB)\n",
      "Trainable params: 52650 (205.66 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "def my_model(in_shape=(28, 28),out_shape=10):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(28, 28)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = my_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question : Afficher les courbes d'apprentissage sur les configurations du MLP suivantes (256,128) (128,64) (64,32) (32,16)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_70\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_8 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_208 (Dense)           (None, 256)               200960    \n",
      "                                                                 \n",
      " dense_209 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_210 (Dense)           (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 235146 (918.54 KB)\n",
      "Trainable params: 235146 (918.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def my_model(in_shape=(28, 28),out_shape=10):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(28, 28)))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(out_shape, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = my_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15, 10)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train[:15,:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 31s 5ms/step - loss: 0.0349 - accuracy: 0.9434 - val_loss: 0.0191 - val_accuracy: 0.9681\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 0.0151 - accuracy: 0.9759 - val_loss: 0.0147 - val_accuracy: 0.9758\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 25s 4ms/step - loss: 0.0108 - accuracy: 0.9829 - val_loss: 0.0147 - val_accuracy: 0.9784\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 21s 4ms/step - loss: 0.0080 - accuracy: 0.9874 - val_loss: 0.0164 - val_accuracy: 0.9781\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 22s 4ms/step - loss: 0.0069 - accuracy: 0.9894 - val_loss: 0.0131 - val_accuracy: 0.9829\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 22s 4ms/step - loss: 0.0056 - accuracy: 0.9916 - val_loss: 0.0147 - val_accuracy: 0.9806\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 22s 4ms/step - loss: 0.0047 - accuracy: 0.9930 - val_loss: 0.0153 - val_accuracy: 0.9808\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 26s 4ms/step - loss: 0.0040 - accuracy: 0.9944 - val_loss: 0.0169 - val_accuracy: 0.9797\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 32s 5ms/step - loss: 0.0036 - accuracy: 0.9949 - val_loss: 0.0189 - val_accuracy: 0.9804\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 20s 3ms/step - loss: 0.0031 - accuracy: 0.9956 - val_loss: 0.0174 - val_accuracy: 0.9827\n",
      "time 252.53191995620728\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0021 - accuracy: 0.9971\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0174 - accuracy: 0.9827\n",
      "train_accuracy : 1.00\n",
      "val_accuracy : 0.98\n"
     ]
    }
   ],
   "source": [
    "model = my_model()\n",
    "model.compile(loss='binary_crossentropy', optimizer=tf.optimizers.Adam(learning_rate=1e-3), metrics=['accuracy'])\n",
    "tic = time.time()\n",
    "history = model.fit(x_train, y_train_cat,validation_data=(x_test,y_test_cat),epochs=10, batch_size=10,verbose=1)\n",
    "tac = time.time()\n",
    "print(f\"time {tac-tic}\")\n",
    "# _, accuracy = model.evaluate(X, y,verbose=1)\n",
    "_, train_accuracy = model.evaluate(x_train, y_train_cat,verbose=1)\n",
    "_, val_accuracy = model.evaluate(x_test, y_test_cat,verbose=1)\n",
    "print(\"train_accuracy : {:.2f}\".format(train_accuracy))\n",
    "print(\"val_accuracy : {:.2f}\".format(val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fbe8371d850>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['loss'], label = 'loss')\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "plt.plot(history.history['val_loss'], label = 'val loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy/Loss')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
